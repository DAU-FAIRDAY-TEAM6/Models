{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "dm2n66W32eY0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_preprocessor():\n",
        "    def __init__(self,data,filter_user=1,filter_item=5):\n",
        "        self.data = data\n",
        "        self.filter_user = filter_user  # 사용자(session)의 최소 길이를 지정합니다. 기본값은 1입니다.\n",
        "        self.filter_item = filter_item  # 아이템의 최소 평가 수를 지정합니다. 기본값은 5입니다.\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.filter_()  # 데이터를 전처리합니다.\n",
        "        return self.train_test_split()  # 전처리된 데이터를 훈련 및 테스트 세트로 나눕니다.\n",
        "\n",
        "    def filter_(self):\n",
        "        \"\"\"\n",
        "        사용자(session)의 길이가 짧은 경우와 아이템의 평가 수가 적은 경우를 필터링합니다.\n",
        "\n",
        "        :param filter_user: 사용자의 최소 session 길이입니다. 기본값은 1입니다.\n",
        "        :param filter_item: 아이템의 최소 평가 수입니다. 기본값은 5입니다.\n",
        "        :return: 데이터프레임\n",
        "        \"\"\"\n",
        "        session_lengths = self.data.groupby('user_id').size()\n",
        "        self.data = self.data[np.in1d(self.data['user_id'], session_lengths[session_lengths > 1].index)]  # 길이가 2 미만인 session을 필터링합니다.\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "        item_supports = self.data.groupby('business_id').size()  # 각 아이템의 평가 수를 집계합니다.\n",
        "        self.data = self.data[np.in1d(self.data['business_id'], item_supports[item_supports > 5].index)]  # 평가 수가 5 미만인 아이템을 필터링합니다.\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "        \"\"\"한 번의 클릭만 있는 사용자도 필터링합니다. 아이템을 필터링할 때 단일 클릭 사용자가 발생할 수 있기 때문입니다.\"\"\"\n",
        "        session_lengths = self.data.groupby('user_id').size()\n",
        "        self.data = self.data[np.in1d(self.data['user_id'], session_lengths[session_lengths > 1].index)]\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "\n",
        "    def train_test_split(self,time_range=86400):\n",
        "        \"\"\"\n",
        "        훈련 및 테스트 데이터 세트로 분할합니다.\n",
        "\n",
        "        :param time_range: session이 이 기간 내에 있으면 테스트 데이터로 분류됩니다. 기본값은 86400(1일)입니다.\n",
        "        :return: 두 개의 데이터프레임으로 이루어진 튜플\n",
        "        \"\"\"\n",
        "        tmax = self.data['timestamp'].max()\n",
        "        session_tmax = self.data.groupby('user_id')['timestamp'].max()\n",
        "        train = self.data[np.in1d(self.data['user_id'] , session_tmax[session_tmax <= tmax - 86400].index)]\n",
        "        test = self.data[np.in1d(self.data['user_id'] , session_tmax[session_tmax > tmax - 86400].index)]\n",
        "        print(\"훈련 데이터 집계:  session 개수:%d , 아이템 개수:%d , 이벤트 수:%d\"%(train['user_id'].nunique(),train['business_id'].nunique(),len(train)))\n",
        "        \"\"\"\n",
        "        협업 필터링 특성상, 테스트 데이터에 훈련 데이터에 없는 아이템이 포함되어 있으면 해당 아이템을 필터링합니다.\n",
        "        \"\"\"\n",
        "        test = test[np.in1d(test['business_id'], train['business_id'])]\n",
        "        tslength = test.groupby('user_id').size()\n",
        "        test = test[np.in1d(test['user_id'], tslength[tslength >= 2].index)]\n",
        "        print(\"테스트 데이터 집계:  session 개수:%d , 아이템 개수:%d , 이벤트 수:%d\"%(test['user_id'].nunique(),test['business_id'].nunique(),len(test)))\n",
        "\n",
        "        return train\n"
      ],
      "metadata": {
        "id": "isfExTLH2f1p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPR():\n",
        "    '''\n",
        "    parameter\n",
        "    train_sample_size : 훈련 시, 각 양성 샘플당 샘플링할 음성 샘플의 수\n",
        "    test_sample_size : 테스트 시, 각 양성 샘플당 샘플링할 음성 샘플의 수\n",
        "    num_k : 아이템 임베딩의 차원 크기\n",
        "    evaluation_at : recall@n, 즉 양성 샘플이 상위 몇 개여야 올바른 추천으로 간주하는지 지정\n",
        "    '''\n",
        "    def __init__(self, data, n_epochs=10, batch_size=32, train_sample_size=10, test_sample_size=50, num_k=100, evaluation_at=10):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.train_sample_size = train_sample_size\n",
        "        self.test_sample_size = test_sample_size\n",
        "        self.num_k = num_k\n",
        "        self.evaluation_at = evaluation_at\n",
        "\n",
        "        self.data = data\n",
        "        self.num_user = len(self.data['user_id'].unique())\n",
        "        self.num_item = len(self.data['business_id'].unique())\n",
        "        self.num_event = len(self.data)\n",
        "\n",
        "        self.all_item = set(self.data['business_id'].unique())\n",
        "        self.experiment = []\n",
        "\n",
        "        # 아이디가 항상 연속적이지 않기 때문에, 맵을 생성하여 아이디를 정규화합니다.\n",
        "        user_id = self.data['user_id'].unique()\n",
        "        self.user_id_map = {user_id[i] : i for i in range(self.num_user)}\n",
        "        item_id = self.data['business_id'].unique()\n",
        "        self.item_id_map = {item_id[i] : i for i in range(self.num_item)}\n",
        "        training_data = self.data.loc[:,['user_id','business_id']].values\n",
        "        self.training_data = [[self.user_id_map[training_data[i][0]], self.item_id_map[training_data[i][1]]] for i in range(self.num_event)]\n",
        "\n",
        "        # 데이터 전처리\n",
        "        self.split_data()  # 데이터를 훈련 데이터와 테스트 데이터로 분할합니다.\n",
        "        self.sample_dict = self.negative_sample()  # 각 훈련 데이터 (사용자, 아이템+)에 대해 BPR 훈련을 위해 10개의 음성 아이템을 샘플링합니다.\n",
        "\n",
        "        # 모델 생성\n",
        "        self.build_model()  # TensorFlow 그래프를 구축합니다.\n",
        "        self.sess = tf.Session()  # 세션을 생성합니다.\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def split_data(self):\n",
        "        user_session = self.data.groupby('user_id')['business_id'].apply(set).reset_index().loc[:,['business_id']].values.reshape(-1)\n",
        "        self.testing_data = []\n",
        "        for index, session in enumerate(user_session):\n",
        "            random_pick = self.item_id_map[random.sample(session, 1)[0]]\n",
        "            self.training_data.remove([index, random_pick])\n",
        "            self.testing_data.append([index, random_pick])\n",
        "\n",
        "    def negative_sample(self):\n",
        "        user_session = self.data.groupby('user_id')['business_id'].apply(set).reset_index().loc[:,['business_id']].values.reshape(-1)\n",
        "        sample_dict = {}\n",
        "\n",
        "        for td in self.training_data:\n",
        "            sample_dict[tuple(td)] = [self.item_id_map[s] for s in random.sample(self.all_item.difference(user_session[td[0]]), self.train_sample_size)]\n",
        "\n",
        "        return sample_dict\n",
        "\n",
        "    def build_model(self):\n",
        "        self.X_user = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_pos_item = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_neg_item = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_predict = tf.placeholder(tf.int32, shape=(1))\n",
        "\n",
        "        user_embedding = tf.Variable(tf.truncated_normal(shape=[self.num_user, self.num_k], mean=0.0, stddev=0.5))\n",
        "        item_embedding = tf.Variable(tf.truncated_normal(shape=[self.num_item, self.num_k], mean=0.0, stddev=0.5))\n",
        "\n",
        "        embed_user = tf.nn.embedding_lookup(user_embedding, self.X_user)\n",
        "        embed_pos_item = tf.nn.embedding_lookup(item_embedding, self.X_pos_item)\n",
        "        embed_neg_item = tf.nn.embedding_lookup(item_embedding, self.X_neg_item)\n",
        "\n",
        "        pos_score = tf.matmul(embed_user, embed_pos_item, transpose_b=True)\n",
        "        neg_score = tf.matmul(embed_user, embed_neg_item, transpose_b=True)\n",
        "\n",
        "        self.loss = tf.reduce_mean(-tf.log(tf.nn.sigmoid(pos_score - neg_score)))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
        "\n",
        "        predict_user_embed = tf.nn.embedding_lookup(user_embedding, self.X_predict)\n",
        "        self.predict = tf.matmul(predict_user_embed, item_embedding, transpose_b=True)\n",
        "\n",
        "    def fit(self):\n",
        "        self.experiment = []\n",
        "        for epoch in range(self.n_epochs):\n",
        "            np.random.shuffle(self.training_data)\n",
        "            total_loss = 0\n",
        "            for i in range(0, len(self.training_data), self.batch_size):\n",
        "                training_batch = self.training_data[i:i+self.batch_size]\n",
        "                user_id = []\n",
        "                pos_item_id = []\n",
        "                neg_item_id = []\n",
        "                for single_training in training_batch:\n",
        "                    for neg_sample in list(self.sample_dict[tuple(single_training)]):\n",
        "                        user_id.append(single_training[0])\n",
        "                        pos_item_id.append(single_training[1])\n",
        "                        neg_item_id.append(neg_sample)\n",
        "\n",
        "                user_id = np.array(user_id).reshape(-1, 1)\n",
        "                pos_item_id = np.array(pos_item_id).reshape(-1, 1)\n",
        "                neg_item_id = np.array(neg_item_id).reshape(-1, 1)\n",
        "\n",
        "                _, loss = self.sess.run([self.optimizer, self.loss],\n",
        "                            feed_dict={self.X_user: user_id, self.X_pos_item: pos_item_id, self.X_neg_item: neg_item_id}\n",
        "                            )\n",
        "                total_loss += loss\n",
        "\n",
        "            num_true = 0\n",
        "            for test in self.testing_data:\n",
        "                result = self.sess.run(self.predict, feed_dict={self.X_predict: [test[0]]})\n",
        "                result = result.reshape(-1)\n",
        "                if (result[[self.item_id_map[s] for s in random.sample(self.all_item, self.test_sample_size)]] > result[test[1]]).sum() + 1 <= self.evaluation_at:\n",
        "                    num_true += 1\n",
        "\n",
        "            print(\"epoch:%d , loss:%.2f , recall:%.2f\" % (epoch, total_loss, num_true/len(self.testing_data)))\n",
        "            self.experiment.append([epoch, total_loss, num_true/len(self.testing_data)])\n"
      ],
      "metadata": {
        "id": "L-zsMbIE2qXD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data = pd.read_csv('ratings_small.csv')\n",
        "    dp = Data_preprocessor(data)\n",
        "    processed_data = dp.preprocess()\n",
        "\n",
        "    bpr = BPR(processed_data)\n",
        "    bpr.fit()"
      ],
      "metadata": {
        "id": "idXmIszB2vG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}