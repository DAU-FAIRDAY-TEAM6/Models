{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSwsO3g1k-Ju",
        "outputId": "dacdad68-ef45-49f2-ad3a-5874e482ba5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dm2n66W32eY0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "isfExTLH2f1p"
      },
      "outputs": [],
      "source": [
        "class Data_preprocessor():\n",
        "    def __init__(self,data,filter_user=1,filter_item=5):\n",
        "        self.data = data\n",
        "        self.filter_user = filter_user  # 사용자(session)의 최소 길이를 지정합니다. 기본값은 1입니다.\n",
        "        self.filter_item = filter_item  # 아이템의 최소 평가 수를 지정합니다. 기본값은 5입니다.\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.filter_()  # 데이터를 전처리합니다.\n",
        "        return self.train_test_split()  # 전처리된 데이터를 훈련 및 테스트 세트로 나눕니다.\n",
        "\n",
        "    def filter_(self):\n",
        "        \"\"\"\n",
        "        사용자(session)의 길이가 짧은 경우와 아이템의 평가 수가 적은 경우를 필터링합니다.\n",
        "\n",
        "        :param filter_user: 사용자의 최소 session 길이입니다. 기본값은 1입니다.\n",
        "        :param filter_item: 아이템의 최소 평가 수입니다. 기본값은 5입니다.\n",
        "        :return: 데이터프레임\n",
        "        \"\"\"\n",
        "        session_lengths = self.data.groupby('user_id').size()\n",
        "        self.data = self.data[np.in1d(self.data['user_id'], session_lengths[session_lengths > 1].index)]  # 길이가 2 미만인 session을 필터링합니다.\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "        item_supports = self.data.groupby('business_id').size()  # 각 아이템의 평가 수를 집계합니다.\n",
        "        self.data = self.data[np.in1d(self.data['business_id'], item_supports[item_supports > 5].index)]  # 평가 수가 5 미만인 아이템을 필터링합니다.\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "        \"\"\"한 번의 클릭만 있는 사용자도 필터링합니다. 아이템을 필터링할 때 단일 클릭 사용자가 발생할 수 있기 때문입니다.\"\"\"\n",
        "        session_lengths = self.data.groupby('user_id').size()\n",
        "        self.data = self.data[np.in1d(self.data['user_id'], session_lengths[session_lengths > 1].index)]\n",
        "        print(\"남은 데이터: %d\"%(len(self.data)))\n",
        "\n",
        "    def train_test_split(self,time_range=86400):\n",
        "        \"\"\"\n",
        "        훈련 및 테스트 데이터 세트로 분할합니다.\n",
        "\n",
        "        :param time_range: session이 이 기간 내에 있으면 테스트 데이터로 분류됩니다. 기본값은 86400(1일)입니다.\n",
        "        :return: 두 개의 데이터프레임으로 이루어진 튜플\n",
        "        \"\"\"\n",
        "        tmax = self.data['date'].max()\n",
        "        session_tmax = self.data.groupby('user_id')['date'].max()\n",
        "        train = self.data[np.in1d(self.data['user_id'] , session_tmax[session_tmax <= tmax - 86400].index)]\n",
        "        test = self.data[np.in1d(self.data['user_id'] , session_tmax[session_tmax > tmax - 86400].index)]\n",
        "        print(\"훈련 데이터 집계:  session 개수:%d , 아이템 개수:%d , 이벤트 수:%d\"%(train['user_id'].nunique(),train['business_id'].nunique(),len(train)))\n",
        "        \"\"\"\n",
        "        협업 필터링 특성상, 테스트 데이터에 훈련 데이터에 없는 아이템이 포함되어 있으면 해당 아이템을 필터링합니다.\n",
        "        \"\"\"\n",
        "        test = test[np.in1d(test['business_id'], train['business_id'])]\n",
        "        tslength = test.groupby('user_id').size()\n",
        "        test = test[np.in1d(test['user_id'], tslength[tslength >= 2].index)]\n",
        "        print(\"테스트 데이터 집계:  session 개수:%d , 아이템 개수:%d , 이벤트 수:%d\"%(test['user_id'].nunique(),test['business_id'].nunique(),len(test)))\n",
        "\n",
        "        return train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "L-zsMbIE2qXD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class BPR():\n",
        "    '''\n",
        "    parameter\n",
        "    train_sample_size : 훈련 시, 각 양성 샘플당 샘플링할 음성 샘플의 수\n",
        "    test_sample_size : 테스트 시, 각 양성 샘플당 샘플링할 음성 샘플의 수\n",
        "    num_k : 아이템 임베딩의 차원 크기\n",
        "    evaluation_at : recall@n, 즉 양성 샘플이 상위 몇 개여야 올바른 추천으로 간주하는지 지정\n",
        "    '''\n",
        "    def __init__(self, data, n_epochs=10, batch_size=32, train_sample_size=10, test_sample_size=50, num_k=100, evaluation_at=10):\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.train_sample_size = train_sample_size\n",
        "        self.test_sample_size = test_sample_size\n",
        "        self.num_k = num_k\n",
        "        self.evaluation_at = evaluation_at\n",
        "\n",
        "        self.data = data\n",
        "        self.num_user = len(self.data['user_id'].unique())\n",
        "        self.num_item = len(self.data['business_id'].unique())\n",
        "        self.num_event = len(self.data)\n",
        "\n",
        "        self.all_item = set(self.data['business_id'].unique())\n",
        "        self.experiment = []\n",
        "\n",
        "        # 아이디가 항상 연속적이지 않기 때문에, 맵을 생성하여 아이디를 정규화합니다.\n",
        "        user_id = self.data['user_id'].unique()\n",
        "        self.user_id_map = {user_id[i] : i for i in range(self.num_user)}\n",
        "        item_id = self.data['business_id'].unique()\n",
        "        self.item_id_map = {item_id[i] : i for i in range(self.num_item)}\n",
        "        training_data = self.data.loc[:,['user_id','business_id']].values\n",
        "        self.training_data = [[self.user_id_map[training_data[i][0]], self.item_id_map[training_data[i][1]]] for i in range(self.num_event)]\n",
        "\n",
        "        # 데이터 전처리\n",
        "        self.split_data()  # 데이터를 훈련 데이터와 테스트 데이터로 분할합니다.\n",
        "        self.sample_dict = self.negative_sample()  # 각 훈련 데이터 (사용자, 아이템+)에 대해 BPR 훈련을 위해 10개의 음성 아이템을 샘플링합니다.\n",
        "\n",
        "        # 모델 생성\n",
        "        self.build_model()  # TensorFlow 그래프를 구축합니다.\n",
        "        self.sess = tf.compat.v1.Session()  # 세션을 생성합니다.\n",
        "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    def split_data(self):\n",
        "        user_session = self.data.groupby('user_id')['business_id'].apply(set).reset_index().loc[:,['business_id']].values.reshape(-1)\n",
        "        self.testing_data = []\n",
        "        for index, session in enumerate(user_session):\n",
        "            random_pick = self.item_id_map[random.sample(session, 1)[0]]\n",
        "            self.training_data.remove([index, random_pick])\n",
        "            self.testing_data.append([index, random_pick])\n",
        "\n",
        "    def negative_sample(self):\n",
        "        user_session = self.data.groupby('user_id')['business_id'].apply(set).reset_index().loc[:,['business_id']].values.reshape(-1)\n",
        "        sample_dict = {}\n",
        "\n",
        "        for td in self.training_data:\n",
        "            sample_dict[tuple(td)] = [self.item_id_map[s] for s in random.sample(self.all_item.difference(user_session[td[0]]), self.train_sample_size)]\n",
        "\n",
        "        return sample_dict\n",
        "\n",
        "    def build_model(self):\n",
        "        self.X_user = tf.compat.v1.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_pos_item = tf.compat.v1.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_neg_item = tf.compat.v1.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.X_predict = tf.compat.v1.placeholder(tf.int32, shape=(1))\n",
        "\n",
        "        user_embedding = tf.Variable(tf.random.truncated_normal(shape=[self.num_user, self.num_k], mean=0.0, stddev=0.5))\n",
        "        item_embedding = tf.Variable(tf.random.truncated_normal(shape=[self.num_item, self.num_k], mean=0.0, stddev=0.5))\n",
        "\n",
        "        embed_user = tf.nn.embedding_lookup(user_embedding, self.X_user)\n",
        "        embed_pos_item = tf.nn.embedding_lookup(item_embedding, self.X_pos_item)\n",
        "        embed_neg_item = tf.nn.embedding_lookup(item_embedding, self.X_neg_item)\n",
        "\n",
        "        pos_score = tf.matmul(embed_user, embed_pos_item, transpose_b=True)\n",
        "        neg_score = tf.matmul(embed_user, embed_neg_item, transpose_b=True)\n",
        "\n",
        "        self.loss = tf.reduce_mean(-tf.math.log(tf.nn.sigmoid(pos_score - neg_score)))\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
        "\n",
        "        predict_user_embed = tf.nn.embedding_lookup(user_embedding, self.X_predict)\n",
        "        self.predict = tf.matmul(predict_user_embed, item_embedding, transpose_b=True)\n",
        "\n",
        "    def fit(self):\n",
        "        self.experiment = []\n",
        "        for epoch in range(self.n_epochs):\n",
        "            np.random.shuffle(self.training_data)\n",
        "            total_loss = 0\n",
        "            for i in range(0, len(self.training_data), self.batch_size):\n",
        "                training_batch = self.training_data[i:i+self.batch_size]\n",
        "                user_id = []\n",
        "                pos_item_id = []\n",
        "                neg_item_id = []\n",
        "                for single_training in training_batch:\n",
        "                    for neg_sample in list(self.sample_dict[tuple(single_training)]):\n",
        "                        user_id.append(single_training[0])\n",
        "                        pos_item_id.append(single_training[1])\n",
        "                        neg_item_id.append(neg_sample)\n",
        "\n",
        "                user_id = np.array(user_id).reshape(-1, 1)\n",
        "                pos_item_id = np.array(pos_item_id).reshape(-1, 1)\n",
        "                neg_item_id = np.array(neg_item_id).reshape(-1, 1)\n",
        "\n",
        "                _, loss = self.sess.run([self.optimizer, self.loss],\n",
        "                            feed_dict={self.X_user: user_id, self.X_pos_item: pos_item_id, self.X_neg_item: neg_item_id}\n",
        "                            )\n",
        "                total_loss += loss\n",
        "\n",
        "            num_true = 0\n",
        "            for test in self.testing_data:\n",
        "                result = self.sess.run(self.predict, feed_dict={self.X_predict: [test[0]]})\n",
        "                result = result.reshape(-1)\n",
        "                if (result[[self.item_id_map[s] for s in random.sample(self.all_item, self.test_sample_size)]] > result[test[1]]).sum() + 1 <= self.evaluation_at:\n",
        "                    num_true += 1\n",
        "\n",
        "            print(\"epoch:%d , loss:%.2f , recall:%.2f\" % (epoch, total_loss, num_true/len(self.testing_data)))\n",
        "            self.experiment.append([epoch, total_loss, num_true/len(self.testing_data)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "idXmIszB2vG5",
        "outputId": "681d691f-bd8a-49cd-f7f8-26516af89c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "남은 데이터: 2230\n",
            "남은 데이터: 224\n",
            "남은 데이터: 147\n",
            "훈련 데이터 집계:  session 개수:46 , 아이템 개수:24 , 이벤트 수:145\n",
            "테스트 데이터 집계:  session 개수:1 , 아이템 개수:2 , 이벤트 수:2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-3701fb40ae7e>:50: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  random_pick = self.item_id_map[random.sample(session, 1)[0]]\n",
            "<ipython-input-37-3701fb40ae7e>:59: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  sample_dict[tuple(td)] = [self.item_id_map[s] for s in random.sample(self.all_item.difference(user_session[td[0]]), self.train_sample_size)]\n",
            "<ipython-input-37-3701fb40ae7e>:114: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  if (result[[self.item_id_map[s] for s in random.sample(self.all_item, self.test_sample_size)]] > result[test[1]]).sum() + 1 <= self.evaluation_at:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sample larger than population or is negative",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-7623f894f473>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# BPR 모델을 사용하여 학습합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-3701fb40ae7e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_id_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_at\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mnum_true\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mrandbelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # TXT 파일을 Pandas DataFrame으로 바로 읽어옵니다.\n",
        "    txt_path = '/content/drive/MyDrive/Graduationproject/sample_data.txt'\n",
        "    data = pd.read_csv(txt_path, header=None, names=['user_id', 'business_id', 'stars', 'date'])\n",
        "\n",
        "    dp = Data_preprocessor(data)\n",
        "    processed_data = dp.preprocess()\n",
        "\n",
        "    # BPR 모델을 사용하여 학습합니다.\n",
        "    bpr = BPR(processed_data)\n",
        "    bpr.fit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}