{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411572b3-c849-4886-9a35-7d0351ee6feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/sh0416/bpr/blob/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fefa5d-a2a0-4622-87ee-73496a4b1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from setuptools import setup, Extension\n",
    "# from torch.utils import cpp_extension\n",
    "\n",
    "# setup(name='vsl_cpp',\n",
    "#       ext_modules=[cpp_extension.CppExtension('vsl_cpp', ['vsl_cpp.cpp'])],\n",
    "#       cmdclass={'build_ext': cpp_extension.BuildExtension})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6beca1c1-5581-4ef5-a022-0bfbf25a0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vsl_cpp\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pprint\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from setuptools import setup, Extension\n",
    "\n",
    "import argparse\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import cpp_extension\n",
    "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b50062b-502d-41c0-b3c2-02a34e72d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableShapeList(object):\n",
    "    def __init__(self, indexes, data):\n",
    "        super().__init__()\n",
    "        self.batch_size = indexes.shape[0] - 1\n",
    "        self.indexes = indexes\n",
    "        self.data = data\n",
    "\n",
    "    @classmethod\n",
    "    def from_tensors(cls, tensors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensors (list(torch.LongTensor)): list of tensors which have different shape.\n",
    "                                              For now, operation is only supported for one dimensional tensor.\n",
    "        \"\"\"\n",
    "        assert len(tensors) > 0, \"`tensors` is empty\"\n",
    "        assert all([len(x.shape) == 1 for x in tensors]), \"Some elements in `tensors` are not one dimensional\"\n",
    "        # Initialize `batch_size`\n",
    "        batch_size = len(tensors)\n",
    "        \n",
    "        # Build up `indexes`\n",
    "        indexes = torch.empty((batch_size+1,), dtype=torch.long)\n",
    "        indexes[0], tmp = 0, 0\n",
    "        for idx, tensor in enumerate(tensors, start=1):\n",
    "            tmp += tensor.numel()\n",
    "            indexes[idx] = tmp\n",
    "        \n",
    "        # Build up `data`\n",
    "        data = torch.empty((indexes[batch_size],), dtype=torch.long)\n",
    "        for idx, tensor in enumerate(tensors, start=0):\n",
    "            data[indexes[idx]:indexes[idx+1]] = tensor\n",
    "        return cls(indexes, data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[self.indexes[idx]:self.indexes[idx+1]]\n",
    "    \n",
    "    def get_size_tensor(self):\n",
    "        return self.indexes[1:] - self.indexes[:-1]\n",
    "\n",
    "\n",
    "def vsl_intersection(list1, list2):\n",
    "    \"\"\"Calculate intersection VariableShapeList.\n",
    "    For the first, we doesn't care about the duplicated item in the list.\n",
    "    So, you must check whether the element in the list is not duplicated.\n",
    "    \n",
    "    Args:\n",
    "        list1 (VariableShapeList): \n",
    "        list2 (VariableShapeList):\n",
    "        \n",
    "    Returns:\n",
    "        VariableShapeList\n",
    "    \"\"\"\n",
    "    # get `batch_size`\n",
    "    assert list1.batch_size == list2.batch_size, \"list1 and list2 have different batch size\"\n",
    "    data, indexes = vsl_cpp.vsl_intersection(list1.data,\n",
    "                                             list1.indexes,\n",
    "                                             list2.data,\n",
    "                                             list2.indexes)\n",
    "    return VariableShapeList(indexes, data)\n",
    "    \n",
    "\n",
    "def vsl_precision(pred, true):\n",
    "    \"\"\"Calculate precision.\n",
    "    \n",
    "    Args:\n",
    "        pred (VariableShapeList): \n",
    "        true (variableShapeList): \n",
    "    Returns:\n",
    "        torch.FloatTensor [batch_size]\n",
    "    \"\"\"\n",
    "    if not torch.all(pred.get_size_tensor()>0):\n",
    "        raise ZeroDivisionError(\"The denominator of precision could be zero\")\n",
    "    intersection = vsl_intersection(pred, true)\n",
    "    intersection_size = intersection.get_size_tensor().float()\n",
    "    pred_size = pred.get_size_tensor().float()\n",
    "    return intersection_size / pred_size\n",
    "    \n",
    "    \n",
    "def vsl_recall(pred, true):\n",
    "    \"\"\"Calculate recall.\n",
    "    \n",
    "    Args:\n",
    "        pred (VariableShapeList): \n",
    "        true (variableShapeList): \n",
    "    Returns:\n",
    "        torch.FloatTensor [batch_size]\n",
    "    \"\"\"\n",
    "    if not torch.all(true.get_size_tensor()>0):\n",
    "        raise ZeroDivisionError(\"The denominator of precision could be zero\")\n",
    "    intersection = vsl_intersection(pred, true)\n",
    "    intersection_size = intersection.get_size_tensor().float()\n",
    "    true_size = true.get_size_tensor().float()\n",
    "    return intersection_size / true_size\n",
    "    \n",
    "    \n",
    "def precision_and_recall_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute precision at k using GPU.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) Precision and recall at k\n",
    "    \"\"\"\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "\n",
    "    # Compute all pair of training and test record\n",
    "    result = None\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "        # Create already observed mask\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i+j])).cuda(), value=torch.tensor(0.0).cuda())\n",
    "        # Calculate prediction value\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "    result = result.cpu()\n",
    "    # Sort indice and get test_pred_topk\n",
    "    precisions, recalls = [], []\n",
    "    for k in klist:\n",
    "        precision, recall = 0, 0\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            test = test_user_list[i]\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            val = len(test & pred)\n",
    "            precision += val / min([k, len(test)])\n",
    "            recall += val / len(test)\n",
    "        precisions.append(precision / user_emb.shape[0])\n",
    "        recalls.append(recall / user_emb.shape[0])\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def accuracy(pred, true, total_size):\n",
    "    \"\"\"Calculate accuracy.\n",
    "    \n",
    "    Args:\n",
    "        pred (VariableLengthList): \n",
    "        true (VariableLengthList): \n",
    "        total_size (torch.FloatTensor): [batch_size] or scalar tensor when the `total_size` is equal over all examples.\n",
    "    Returns:\n",
    "        torch.FloatTensor [batch_size]\n",
    "    \"\"\"\n",
    "    raise NotImplemented\n",
    "    \n",
    "\n",
    "def mean_average_precision(pred, true):\n",
    "    raise NotImplemented\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39313b5d-b2e1-43e1-8179-8f911a9a2508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete assigning unique index to user and item\n",
      "Complete spliting items for training and testing\n",
      "Complete creating pair\n"
     ]
    }
   ],
   "source": [
    "class DatasetLoader(object):\n",
    "    def load(self):\n",
    "        \"\"\"Minimum condition for dataset:\n",
    "          * All users must have at least one item record.\n",
    "          * All items must have at least one user record.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MovieLens20M(DatasetLoader):\n",
    "    def __init__(self, data_dir):\n",
    "        self.fpath = os.path.join(data_dir, 'test_set.csv')\n",
    "\n",
    "    def load(self):\n",
    "        df = pd.read_csv(self.fpath,\n",
    "                         sep=',',\n",
    "                         names=['user', 'item', 'rate', 'time'],\n",
    "                         usecols=['user', 'item', 'time'],\n",
    "                         skiprows=1)\n",
    "        return df\n",
    "\n",
    "def convert_unique_idx(df, column_name):\n",
    "    column_dict = {x: i for i, x in enumerate(df[column_name].unique())}\n",
    "    df[column_name] = df[column_name].apply(column_dict.get)\n",
    "    df[column_name] = df[column_name].astype('int')\n",
    "    assert df[column_name].min() == 0\n",
    "    assert df[column_name].max() == len(column_dict) - 1\n",
    "    return df, column_dict\n",
    "\n",
    "\n",
    "def create_user_list(df, user_size):\n",
    "    user_list = [list() for u in range(user_size)]\n",
    "    for row in df.itertuples():\n",
    "        user_list[row.user].append((row.time, row.item))\n",
    "    return user_list\n",
    "\n",
    "\n",
    "def split_train_test(df, user_size, test_size=0.2, time_order=False):\n",
    "    \"\"\"Split a dataset into `train_user_list` and `test_user_list`.\n",
    "    Because it needs `user_list` for splitting dataset as `time_order` is set,\n",
    "    Returning `user_list` data structure will be a good choice.\"\"\"\n",
    "    # TODO: Handle duplicated items\n",
    "    if not time_order:\n",
    "        test_idx = np.random.choice(len(df), size=int(len(df)*test_size))\n",
    "        train_idx = list(set(range(len(df))) - set(test_idx))\n",
    "        test_df = df.loc[test_idx].reset_index(drop=True)\n",
    "        train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "        test_user_list = create_user_list(test_df, user_size)\n",
    "        train_user_list = create_user_list(train_df, user_size)\n",
    "    else:\n",
    "        total_user_list = create_user_list(df, user_size)\n",
    "        train_user_list = [None] * len(user_list)\n",
    "        test_user_list = [None] * len(user_list)\n",
    "        for user, item_list in enumerate(total_user_list):\n",
    "            # Choose latest item\n",
    "            item_list = sorted(item_list, key=lambda x: x[0])\n",
    "            # Split item\n",
    "            test_item = item_list[math.ceil(len(item_list)*(1-test_size)):]\n",
    "            train_item = item_list[:math.ceil(len(item_list)*(1-test_size))]\n",
    "            # Register to each user list\n",
    "            test_user_list[user] = test_item\n",
    "            train_user_list[user] = train_item\n",
    "    # Remove time\n",
    "    test_user_list = [list(map(lambda x: x[1], l)) for l in test_user_list]\n",
    "    train_user_list = [list(map(lambda x: x[1], l)) for l in train_user_list]\n",
    "    return train_user_list, test_user_list\n",
    "\n",
    "\n",
    "def create_pair(user_list):\n",
    "    pair = []\n",
    "    for user, item_list in enumerate(user_list):\n",
    "        pair.extend([(user, item) for item in item_list])\n",
    "    return pair\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    df = MovieLens20M(args.data_dir).load()\n",
    "    df, user_mapping = convert_unique_idx(df, 'user')\n",
    "    df, item_mapping = convert_unique_idx(df, 'item')\n",
    "    print('Complete assigning unique index to user and item')\n",
    "\n",
    "    user_size = len(df['user'].unique())\n",
    "    item_size = len(df['item'].unique())\n",
    "\n",
    "    train_user_list, test_user_list = split_train_test(df,\n",
    "                                                       user_size,\n",
    "                                                       test_size=args.test_size,\n",
    "                                                       time_order=args.time_order)\n",
    "    print('Complete spliting items for training and testing')\n",
    "\n",
    "    train_pair = create_pair(train_user_list)\n",
    "    print('Complete creating pair')\n",
    "\n",
    "    dataset = {'user_size': user_size, 'item_size': item_size, \n",
    "               'user_mapping': user_mapping, 'item_mapping': item_mapping,\n",
    "               'train_user_list': train_user_list, 'test_user_list': test_user_list,\n",
    "               'train_pair': train_pair}\n",
    "    dirname = os.path.dirname(os.path.abspath(args.output_data))\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    with open(args.output_data, 'wb') as f:\n",
    "        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse argument\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--dataset',\n",
    "    #                     choices=['test_set'],\n",
    "    #                     default='test_set',\n",
    "    #                     help=\"Dataset to use\")\n",
    "    # parser.add_argument('--data_dir',\n",
    "    #                     type=str,\n",
    "    #                     default=os.path.join('test_set'),\n",
    "    #                     help=\"File path for raw data\")\n",
    "    # parser.add_argument('--output_data',\n",
    "    #                     type=str,\n",
    "    #                     default=os.path.join('preprocessed', 'test_set.pickle'),\n",
    "    #                     help=\"File path for preprocessed data\")\n",
    "    # parser.add_argument('--test_size',\n",
    "    #                     type=float,\n",
    "    #                     default=0.2,\n",
    "    #                     help=\"Proportion for training and testing split\")\n",
    "    # parser.add_argument('--time_order',\n",
    "    #                     action='store_true',\n",
    "    #                     help=\"Use time order for splitting training and testing data\")\n",
    "    # args = parser.parse_args()\n",
    "    args = argparse.Namespace()\n",
    "    args.dataset = 'test_set'\n",
    "    args.data_dir = os.path.join('data')\n",
    "    args.output_data = os.path.join('preprocessed', 'test_set.pickle')\n",
    "    args.test_size = 0.2\n",
    "    args.time_order = False\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f15841-fb99-42cc-9d5f-cf074c839b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup(name='vsl_op',\n",
    "#       ext_modules=[cpp_extension.CppExtension('vsl_op', ['vsl_op.cpp'])],\n",
    "#       cmdclass={'build_ext': cpp_extension.BuildExtension})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb43907-9cc5-418d-8645-be5950914c33",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'preprocessed\\\\ml-1m.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 263\u001b[0m\n\u001b[0;32m    200\u001b[0m     args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\n\u001b[0;32m    201\u001b[0m     data\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml-1m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    202\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     model\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpr.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# parser = argparse.ArgumentParser()\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# parser.add_argument('--data',\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m#                     type=str,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m#                     help=\"File path for model\")\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# args = parser.parse_args()\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 152\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    149\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Load preprocess data\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    153\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    154\u001b[0m     user_size, item_size \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_size\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'preprocessed\\\\ml-1m.pickle'"
     ]
    }
   ],
   "source": [
    "class TripletUniformPair(IterableDataset):\n",
    "    def __init__(self, num_item, user_list, pair, shuffle, num_epochs):\n",
    "        self.num_item = num_item\n",
    "        self.user_list = user_list\n",
    "        self.pair = pair\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        # Shuffle per epoch\n",
    "        self.example_size = self.num_epochs * len(self.pair)\n",
    "        self.example_index_queue = deque([])\n",
    "        self.seed = 0\n",
    "        if worker_info is not None:\n",
    "            self.start_list_index = worker_info.id\n",
    "            self.num_workers = worker_info.num_workers\n",
    "            self.index = worker_info.id\n",
    "        else:\n",
    "            self.start_list_index = None\n",
    "            self.num_workers = 1\n",
    "            self.index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.example_size:\n",
    "            raise StopIteration\n",
    "        # If `example_index_queue` is used up, replenish this list.\n",
    "        while len(self.example_index_queue) == 0:\n",
    "            index_list = list(range(len(self.pair)))\n",
    "            if self.shuffle:\n",
    "                random.Random(self.seed).shuffle(index_list)\n",
    "                self.seed += 1\n",
    "            if self.start_list_index is not None:\n",
    "                index_list = index_list[self.start_list_index::self.num_workers]\n",
    "                # Calculate next start index\n",
    "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
    "            self.example_index_queue.extend(index_list)\n",
    "        result = self._example(self.example_index_queue.popleft())\n",
    "        self.index += self.num_workers\n",
    "        return result\n",
    "\n",
    "    def _example(self, idx):\n",
    "        u = self.pair[idx][0]\n",
    "        i = self.pair[idx][1]\n",
    "        j = np.random.randint(self.num_item)\n",
    "        while j in self.user_list[u]:\n",
    "            j = np.random.randint(self.num_item)\n",
    "        return u, i, j\n",
    "\n",
    "\n",
    "class BPR(nn.Module):\n",
    "    def __init__(self, user_size, item_size, dim, weight_decay):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(user_size, dim))\n",
    "        self.H = nn.Parameter(torch.empty(item_size, dim))\n",
    "        nn.init.xavier_normal_(self.W.data)\n",
    "        nn.init.xavier_normal_(self.H.data)\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"Return loss value.\n",
    "        \n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
    "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
    "        \n",
    "        Returns:\n",
    "            torch.FloatTensor\n",
    "        \"\"\"\n",
    "        u = self.W[u, :]\n",
    "        i = self.H[i, :]\n",
    "        j = self.H[j, :]\n",
    "        x_ui = torch.mul(u, i).sum(dim=1)\n",
    "        x_uj = torch.mul(u, j).sum(dim=1)\n",
    "        x_uij = x_ui - x_uj\n",
    "        log_prob = F.logsigmoid(x_uij).sum()\n",
    "        regularization = self.weight_decay * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
    "        return -log_prob + regularization\n",
    "\n",
    "    def recommend(self, u):\n",
    "        \"\"\"Return recommended item list given users.\n",
    "\n",
    "        Args:\n",
    "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
    "\n",
    "        Returns:\n",
    "            pred(torch.LongTensor): recommended item list sorted by preference. [batch_size, item_size]\n",
    "        \"\"\"\n",
    "        u = self.W[u, :]\n",
    "        x_ui = torch.mm(u, self.H.t())\n",
    "        pred = torch.argsort(x_ui, dim=1)\n",
    "        return pred\n",
    "\n",
    "\n",
    "def precision_and_recall_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
    "    \"\"\"Compute precision at k using GPU.\n",
    "\n",
    "    Args:\n",
    "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
    "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
    "        train_user_list (list(set)):\n",
    "        test_user_list (list(set)):\n",
    "        k (list(int)):\n",
    "    Returns:\n",
    "        (torch.Tensor, torch.Tensor) Precision and recall at k\n",
    "    \"\"\"\n",
    "    # Calculate max k value\n",
    "    max_k = max(klist)\n",
    "\n",
    "    # Compute all pair of training and test record\n",
    "    result = None\n",
    "    for i in range(0, user_emb.shape[0], batch):\n",
    "        # Create already observed mask\n",
    "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
    "        for j in range(batch):\n",
    "            if i+j >= user_emb.shape[0]:\n",
    "                break\n",
    "            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i+j])).cuda(), value=torch.tensor(0.0).cuda())\n",
    "        # Calculate prediction value\n",
    "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
    "        cur_result = torch.sigmoid(cur_result)\n",
    "        assert not torch.any(torch.isnan(cur_result))\n",
    "        # Make zero for already observed item\n",
    "        cur_result = torch.mul(mask, cur_result)\n",
    "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
    "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
    "\n",
    "    result = result.cpu()\n",
    "    # Sort indice and get test_pred_topk\n",
    "    precisions, recalls = [], []\n",
    "    for k in klist:\n",
    "        precision, recall = 0, 0\n",
    "        for i in range(user_emb.shape[0]):\n",
    "            test = set(test_user_list[i])\n",
    "            pred = set(result[i, :k].numpy().tolist())\n",
    "            val = len(test & pred)\n",
    "            precision += val / max([min([k, len(test)]), 1])\n",
    "            recall += val / max([len(test), 1])\n",
    "        precisions.append(precision / user_emb.shape[0])\n",
    "        recalls.append(recall / user_emb.shape[0])\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Initialize seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # Load preprocess data\n",
    "    with open(args.data, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        user_size, item_size = dataset['user_size'], dataset['item_size']\n",
    "        train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
    "        train_pair = dataset['train_pair']\n",
    "    print('Load complete')\n",
    "\n",
    "    # Create dataset, model, optimizer\n",
    "    dataset = TripletUniformPair(item_size, train_user_list, train_pair, True, args.n_epochs)\n",
    "    loader = DataLoader(dataset, batch_size=args.batch_size, num_workers=16)\n",
    "    model = BPR(user_size, item_size, args.dim, args.weight_decay).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training\n",
    "    smooth_loss = 0\n",
    "    idx = 0\n",
    "    for u, i, j in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u, i, j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('train/loss', loss, idx)\n",
    "        smooth_loss = smooth_loss*0.99 + loss*0.01\n",
    "        if idx % args.print_every == (args.print_every - 1):\n",
    "            print('loss: %.4f' % smooth_loss)\n",
    "        if idx % args.eval_every == (args.eval_every - 1):\n",
    "            plist, rlist = precision_and_recall_k(model.W.detach(),\n",
    "                                                    model.H.detach(),\n",
    "                                                    train_user_list,\n",
    "                                                    test_user_list,\n",
    "                                                    klist=[1, 5, 10])\n",
    "            print('P@1: %.4f, P@5: %.4f P@10: %.4f, R@1: %.4f, R@5: %.4f, R@10: %.4f' % (plist[0], plist[1], plist[2], rlist[0], rlist[1], rlist[2]))\n",
    "            writer.add_scalars('eval', {'P@1': plist[0],\n",
    "                                                    'P@5': plist[1],\n",
    "                                                    'P@10': plist[2]}, idx)\n",
    "            writer.add_scalars('eval', {'R@1': rlist[0],\n",
    "                                                'R@5': rlist[1],\n",
    "                                                'R@10': rlist[2]}, idx)\n",
    "        if idx % args.save_every == (args.save_every - 1):\n",
    "            dirname = os.path.dirname(os.path.abspath(args.model))\n",
    "            os.makedirs(dirname, exist_ok=True)\n",
    "            torch.save(model.state_dict(), args.model)\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Parse argument\n",
    "    args = argparse.Namespace(\n",
    "    data=os.path.join('preprocessed', 'ml-1m.pickle'),\n",
    "    seed=0,\n",
    "    dim=4,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.025,\n",
    "    n_epochs=500,\n",
    "    batch_size=4096,\n",
    "    print_every=20,\n",
    "    eval_every=1000,\n",
    "    save_every=10000,\n",
    "    model=os.path.join('output', 'bpr.pt')\n",
    ")\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--data',\n",
    "    #                     type=str,\n",
    "    #                     default=os.path.join('preprocessed', 'ml-1m.pickle'),\n",
    "    #                     help=\"File path for data\")\n",
    "    # # Seed\n",
    "    # parser.add_argument('--seed',\n",
    "    #                     type=int,\n",
    "    #                     default=0,\n",
    "    #                     help=\"Seed (For reproducability)\")\n",
    "    # # Model\n",
    "    # parser.add_argument('--dim',\n",
    "    #                     type=int,\n",
    "    #                     default=4,\n",
    "    #                     help=\"Dimension for embedding\")\n",
    "    # # Optimizer\n",
    "    # parser.add_argument('--lr',\n",
    "    #                     type=float,\n",
    "    #                     default=1e-3,\n",
    "    #                     help=\"Learning rate\")\n",
    "    # parser.add_argument('--weight_decay',\n",
    "    #                     type=float,\n",
    "    #                     default=0.025,\n",
    "    #                     help=\"Weight decay factor\")\n",
    "    # # Training\n",
    "    # parser.add_argument('--n_epochs',\n",
    "    #                     type=int,\n",
    "    #                     default=500,\n",
    "    #                     help=\"Number of epoch during training\")\n",
    "    # parser.add_argument('--batch_size',\n",
    "    #                     type=int,\n",
    "    #                     default=4096,\n",
    "    #                     help=\"Batch size in one iteration\")\n",
    "    # parser.add_argument('--print_every',\n",
    "    #                     type=int,\n",
    "    #                     default=20,\n",
    "    #                     help=\"Period for printing smoothing loss during training\")\n",
    "    # parser.add_argument('--eval_every',\n",
    "    #                     type=int,\n",
    "    #                     default=1000,\n",
    "    #                     help=\"Period for evaluating precision and recall during training\")\n",
    "    # parser.add_argument('--save_every',\n",
    "    #                     type=int,\n",
    "    #                     default=10000,\n",
    "    #                     help=\"Period for saving model during training\")\n",
    "    # parser.add_argument('--model',\n",
    "    #                     type=str,\n",
    "    #                     default=os.path.join('output', 'bpr.pt'),\n",
    "    #                     help=\"File path for model\")\n",
    "    # args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
