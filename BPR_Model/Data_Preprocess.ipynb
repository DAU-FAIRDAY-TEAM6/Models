{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21166128-d366-4bb6-8b2a-c910d7ce9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "json_path = './yelp/'\n",
    "data_path = './dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c468b5",
   "metadata": {},
   "source": [
    "## Review 데이터 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fb919",
   "metadata": {},
   "source": [
    "user_id 및 business_id 추출, 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca32dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 데이터가 user_id와 business_id를 기준으로 정렬되어 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "json_file_path = json_path + 'review.json'\n",
    "\n",
    "# 유저 ID와 비즈니스 ID를 매핑하기 위한 딕셔너리\n",
    "user_id_mapping = {}\n",
    "business_id_mapping = {}\n",
    "next_user_index = 0\n",
    "next_business_index = 0\n",
    "\n",
    "# 최종 데이터를 리스트에 저장\n",
    "sort_data = []\n",
    "\n",
    "# JSON 파일을 읽고 처리\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    for line in json_file:\n",
    "        data = json.loads(line)\n",
    "        user_id = data['user_id']\n",
    "        business_id = data['business_id']\n",
    "\n",
    "        # 유저 ID 매핑 업데이트\n",
    "        if user_id not in user_id_mapping:\n",
    "            user_id_mapping[user_id] = next_user_index\n",
    "            next_user_index += 1\n",
    "        # 비즈니스 ID 매핑 업데이트\n",
    "        if business_id not in business_id_mapping:\n",
    "            business_id_mapping[business_id] = next_business_index\n",
    "            next_business_index += 1\n",
    "\n",
    "        # 매핑된 ID로 데이터 저장\n",
    "        sort_data.append([user_id_mapping[user_id], business_id_mapping[business_id]])\n",
    "\n",
    "# 데이터를 정렬하고 새 파일에 저장\n",
    "sort_data.sort(key=lambda x: (x[0], x[1]))  # 먼저 user_id, 다음 business_id 기준으로 정렬\\\n",
    "\n",
    "print(\"모든 데이터가 user_id와 business_id를 기준으로 정렬되어 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aacca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필라델피아의 비즈니스 리뷰 데이터가 준비되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필라델피아에 있는 비즈니스 ID를 추출하기 위한 파일 경로\n",
    "business_json_path = json_path + 'business.json'\n",
    "\n",
    "# 필라델피아 비즈니스 ID 저장을 위한 세트\n",
    "philadelphia_business_ids = set()\n",
    "\n",
    "# business.json 파일 읽기\n",
    "with open(business_json_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_data = json.loads(line)\n",
    "        if business_data['city'] == 'Philadelphia':\n",
    "            philadelphia_business_ids.add(business_data['business_id'])\n",
    "\n",
    "# 필라델피아 비즈니스 리뷰만 포함하는 데이터 추출\n",
    "philadelphia_sort_data = [\n",
    "    [user_id, business_id] for user_id, business_id in sort_data\n",
    "    if business_id_mapping.get(business_id) in philadelphia_business_ids\n",
    "]\n",
    "\n",
    "# 필요하다면 philadelphia_sort_data를 파일에 저장할 수 있습니다.\n",
    "\n",
    "print(\"필라델피아의 비즈니스 리뷰 데이터가 준비되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b1806",
   "metadata": {},
   "source": [
    "리뷰가 10개 이상인 유저만 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c1d515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id  business_id\n",
      "0              0            0\n",
      "1              0          473\n",
      "2              0          927\n",
      "3              0        15261\n",
      "4              0        15753\n",
      "...          ...          ...\n",
      "6648468  1676360       114399\n",
      "6648469  1676360       114399\n",
      "6648470  1676360       114399\n",
      "6648471  1676360       114399\n",
      "6648472  1676360       114399\n",
      "\n",
      "[3303811 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# sort_data DataFrame으로 변환\n",
    "sort_data = pd.DataFrame(sort_data, columns=['user_id', 'business_id'])\n",
    "\n",
    "# 각 ID의 출현 빈도를 계산\n",
    "frequency = sort_data['user_id'].value_counts()\n",
    "\n",
    "# 출현 빈도가 10 이상인 ID만 필터링\n",
    "valid_ids = frequency[frequency >= 10].index\n",
    "\n",
    "# 필터링된 ID에 해당하는 데이터만 선택\n",
    "filtered_data = sort_data[sort_data['user_id'].isin(valid_ids)]\n",
    "\n",
    "# 혹은 출력하여 확인\n",
    "print(filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ea305",
   "metadata": {},
   "source": [
    "user_id, business_id 인덱스 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5e60e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17500\\2167955209.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['user_id'] = filtered_data['user_id'].map(user_id_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id  business_id\n",
      "0              0            0\n",
      "1              0          473\n",
      "2              0          927\n",
      "3              0        15261\n",
      "4              0        15753\n",
      "...          ...          ...\n",
      "6648468   117369       114399\n",
      "6648469   117369       114399\n",
      "6648470   117369       114399\n",
      "6648471   117369       114399\n",
      "6648472   117369       114399\n",
      "\n",
      "[3303811 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_17500\\2167955209.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_data['business_id'] = filtered_data['business_id'].map(business_id_mapping_uni)\n"
     ]
    }
   ],
   "source": [
    "philadelphia = []\n",
    "\n",
    "# business.json 파일 읽기\n",
    "with open(business_json_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        business_data = json.loads(line)\n",
    "        if business_data['city'] == 'Philadelphia':\n",
    "            philadelphia_business_id = business_data['business_id']\n",
    "            philadelphia_latitude = business_data['latitude']\n",
    "            philadelphia_longitude = business_data['longitude']\n",
    "            philadelphia.append({'business_id': philadelphia_business_id, \n",
    "                                 'latitude': philadelphia_latitude, \n",
    "                                 'longitude': philadelphia_longitude})\n",
    "            \n",
    "philadelphia = pd.DataFrame(philadelphia)\n",
    "\n",
    "philadelphia['business_id'] = philadelphia['business_id'].map(business_id_mapping)\n",
    "\n",
    "\n",
    "# 유저 ID에 대해 고유한 ID를 정렬된 리스트로 추출하고 0부터 시작하는 새로운 인덱스를 매핑\n",
    "unique_user_ids = sorted(filtered_data['user_id'].unique())\n",
    "user_id_mapping = {id: index for index, id in enumerate(unique_user_ids)}\n",
    "filtered_data['user_id'] = filtered_data['user_id'].map(user_id_mapping)\n",
    "\n",
    "# 비즈니스 ID에 대해 고유한 ID를 정렬된 리스트로 추출하고 0부터 시작하는 새로운 인덱스를 매핑\n",
    "print(filtered_data)\n",
    "unique_business_ids = sorted(filtered_data['business_id'].unique())\n",
    "business_id_mapping_uni = {id: index for index, id in enumerate(unique_business_ids)}\n",
    "filtered_data['business_id'] = filtered_data['business_id'].map(business_id_mapping_uni)\n",
    "\n",
    "philadelphia['business_id'] = philadelphia['business_id'].map(business_id_mapping)\n",
    "\n",
    "# CSV로 저장\n",
    "philadelphia.to_csv('poi.csv', index=False)\n",
    "\n",
    "# 중복 제거\n",
    "indexing_data = filtered_data.drop_duplicates(subset=['user_id', 'business_id'])\n",
    "\n",
    "# # 최종 데이터 프레임 출력하여 확인\n",
    "# print(indexing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d63adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "philadelphia['business_id'] = philadelphia['business_id'].map(business_id_mapping)\n",
    "\n",
    "# CSV로 저장\n",
    "philadelphia.to_csv('poi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506279b7",
   "metadata": {},
   "source": [
    "train 및 test 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4960e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'train.csv'와 'test.csv' 파일이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# drop_duplicates 함수를 사용하여, 각 사용자 ID와 아이템 ID 조합의 첫 번째 등장만 유지합니다.\n",
    "first_data = indexing_data.drop_duplicates(subset=['user_id', 'business_id'])\n",
    "\n",
    "# 각 사용자별로 첫 번째 데이터를 테스트 데이터로, 나머지를 트레이닝 데이터로 분리\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# 사용자별로 그룹화하여 처리\n",
    "for user_id, group in indexing_data.groupby('user_id'):\n",
    "    test_data.append(group.iloc[0].tolist())  # 첫 번째 행을 테스트 데이터에 추가, 리스트로 변환\n",
    "    train_data.extend(group.iloc[1:].values.tolist())  # 나머지 행을 트레이닝 데이터에 추가, 리스트로 변환\n",
    "\n",
    "# 리스트를 DataFrame으로 변환\n",
    "train_df = pd.DataFrame(train_data, columns=['user_id', 'business_id'])\n",
    "test_df = pd.DataFrame(test_data, columns=['user_id', 'business_id'])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "train_df.to_csv(data_path + 'train.csv', index=False, header=False, sep = '\\t')\n",
    "test_df.to_csv(data_path + 'test.csv', index=False, header=False, sep = '\\t')\n",
    "\n",
    "print(\"'train.csv'와 'test.csv' 파일이 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5150d",
   "metadata": {},
   "source": [
    "negative 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d36fc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저와 아이템 집합 구하기\n",
    "users = indexing_data['user_id'].unique()\n",
    "businesses = indexing_data['business_id'].unique()\n",
    "\n",
    "# 결과를 저장할 딕셔너리\n",
    "neg_dict = {}\n",
    "\n",
    "# 각 유저마다 가지고 있지 않은 아이템 찾기\n",
    "for user in users:\n",
    "    user_businesses = indexing_data[indexing_data['user_id'] == user]['business_id'].unique()\n",
    "    not_having_businesses = np.setdiff1d(businesses, user_businesses)\n",
    "    if len(not_having_businesses) > 100:\n",
    "        sampled_businesses = np.random.choice(not_having_businesses, 100, replace=False)\n",
    "    else:\n",
    "        sampled_businesses = not_having_businesses\n",
    "    neg_dict[user] = ','.join([str(item) for item in sampled_businesses])\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "with open(data_path + 'test_neg.csv', 'w') as f:\n",
    "    for user, businesses in neg_dict.items():\n",
    "        f.write(f\"{businesses}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9690b8c",
   "metadata": {},
   "source": [
    "negative 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3015fa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 'test_neg_100.csv'가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 파일을 읽고 쓰기 위해 open 함수를 사용\n",
    "with open(data_path + 'test_neg.csv', 'r') as neg_file, open(data_path + 'test.csv', 'r') as test_file, open(data_path + 'test_neg_100.csv', 'w') as result_file:\n",
    "    neg_lines = neg_file.readlines()\n",
    "    test_lines = test_file.readlines()\n",
    "    \n",
    "    for neg_line, test_line in zip(neg_lines, test_lines):\n",
    "        user_id, neg_data = neg_line.strip().split(',', 1)\n",
    "        test_index, test_value = test_line.strip().split('\\t')\n",
    "        test_tuple = f'({test_index},{test_value})'\n",
    "        \n",
    "        neg_data_formatted = '\\t'.join(neg_data.split(','))\n",
    "        \n",
    "        result_line = test_tuple + '\\t' + neg_data_formatted + '\\n'\n",
    "        result_file.write(result_line)\n",
    "\n",
    "print(\"파일 'test_neg_100.csv'가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f34a8ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 434)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# JSON 파일 불러오기\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(business_json_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 'city'가 'Philadelphia'인 데이터 필터링\u001b[39;00m\n\u001b[0;32m      8\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m [entry \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhiladelphia\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 434)"
     ]
    }
   ],
   "source": [
    "# business_json_path = json_path + 'business.json'\n",
    "\n",
    "# # JSON 파일 불러오기\n",
    "# with open(business_json_path, 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # 'city'가 'Philadelphia'인 데이터 필터링\n",
    "# filtered_data = [entry for entry in data if entry.get('city') == 'Philadelphia']\n",
    "\n",
    "# # 필요한 열만 선택\n",
    "# selected_data = [{'business_id': entry['business_id'], \n",
    "#                   'latitude': entry['latitude'], \n",
    "#                   'longitude': entry['longitude']} for entry in filtered_data]\n",
    "\n",
    "# # business_id를 매핑하여 index로 변경\n",
    "# unique_business_ids = [entry['business_id'] for entry in selected_data]\n",
    "# business_id_mapping = {id: index for index, id in enumerate(unique_business_ids)}\n",
    "\n",
    "# # business_id를 index로 변경\n",
    "# for entry in selected_data:\n",
    "#     entry['business_id'] = business_id_mapping[entry['business_id']]\n",
    "\n",
    "# # DataFrame 생성\n",
    "# df = pd.DataFrame(selected_data)\n",
    "\n",
    "# # CSV로 저장\n",
    "# df.to_csv('poi.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
