{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50bd747-1eb2-42cb-ba84-228df02baaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/uestc7d/pytorch-BPR/blob/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05db66af-f15a-4f7f-9641-fc1c3fabf6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7883fba6-6b50-4c1d-a4b9-a5cfd505f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import argparse\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951051bc-c14f-4aba-920c-5d611998f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFbpr(nn.Module):\n",
    "    \n",
    "    def __init__(self, dataset, factors, learning_rate, reg, init_mean, init_stdev):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        super(MFbpr, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.train = dataset.train\n",
    "        self.test = dataset.test\n",
    "        self.num_user = dataset.num_user\n",
    "        self.num_item = dataset.num_item\n",
    "        self.neg = dataset.neg\n",
    "        self.factors = factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "        self.init_mean = init_mean\n",
    "        self.init_stdev = init_stdev\n",
    "\n",
    "        # user & item latent vectors\n",
    "        self.U = torch.normal(mean = self.init_mean * torch.ones(self.num_user, self.factors), std = self.init_stdev).requires_grad_()\n",
    "        self.V = torch.normal(mean = self.init_mean * torch.ones(self.num_item, self.factors), std = self.init_stdev).requires_grad_()\n",
    "\n",
    "        # optim\n",
    "        self.mf_optim = optim.Adam([self.U, self.V], lr=self.learning_rate)\n",
    "        # self.mf_scheduler = optim.lr_scheduler.MultiStepLR(self.mf_optim, [30, 200], 0.1)\n",
    "        # Each element is the set of items for a user, used for negative sampling\n",
    "        self.items_of_user = []\n",
    "        self.num_rating = 0     # number of ratings\n",
    "        for u in range(len(self.train)):  # xrange를 range로 수정\n",
    "            self.items_of_user.append(set([]))  # sets 모듈을 사용하는 대신 set을 직접 사용\n",
    "            for i in range(len(self.train[u])):  # xrange를 range로 수정\n",
    "                item = self.train[u][i][0]\n",
    "                self.items_of_user[u].add(item)\n",
    "                self.num_rating += 1\n",
    "\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        '''\n",
    "        Args:\n",
    "            u: user id. type=int or list.\n",
    "            i: positive item id. type=int or list.\n",
    "            j: negative item id. type=int or list.\n",
    "\n",
    "        Returns:\n",
    "            y_ui: predicted score between user and positive item.\n",
    "            y_uj: predicted score between user and negative item.\n",
    "            loss: BPR loss. It is the opposite of BPR-OPT.\n",
    "        '''\n",
    "        y_ui = torch.diag(torch.mm(self.U[u], self.V[i].t()))\n",
    "        y_uj = torch.diag(torch.mm(self.U[u], self.V[j].t()))\n",
    "        regularizer = self.reg * (torch.sum(self.U[u] ** 2) + torch.sum(self.V[i] ** 2) + torch.sum(self.V[j] ** 2))\n",
    "        loss = regularizer - torch.sum(torch.log2(torch.sigmoid(y_ui - y_uj)))\n",
    "        return y_ui, y_uj, loss\n",
    "\n",
    "    def build_model(self, maxIter=100, num_thread=4, batch_size=32):\n",
    "        # dataloader\n",
    "        data_loader = DataLoader(self.dataset, batch_size=batch_size)\n",
    "\n",
    "        # Training process\n",
    "        print(\"Training MF-BPR with: learning_rate={:.4f}, regularization={:.4f}, factors={}, #epoch={}, batch_size={}.\".format(\n",
    "            self.learning_rate, self.reg, self.factors, maxIter, batch_size))  # 수정된 print 문\n",
    "        t1 = time.time()\n",
    "        iter_loss = 0\n",
    "        for iteration in range(maxIter):  # xrange를 range로 수정\n",
    "            # self.mf_scheduler.step()\n",
    "            # Each training epoch\n",
    "            for s, (users, items_pos, items_neg) in enumerate(data_loader):\n",
    "                # sample a batch of users, positive samples and negative samples\n",
    "\n",
    "                # zero grad\n",
    "                self.mf_optim.zero_grad()\n",
    "                # forward propagation\n",
    "                y_ui, y_uj, loss = self.forward(users, items_pos, items_neg)\n",
    "                iter_loss += loss\n",
    "                # back propagation\n",
    "                loss.backward()\n",
    "                self.mf_optim.step()\n",
    "\n",
    "            # check performance\n",
    "            if iteration % 20 == 19:\n",
    "                t2 = time.time()\n",
    "                topK = 20\n",
    "                (hits, ndcgs) = evaluate_model(self, self.test, topK, num_thread)\n",
    "                # save the hr and ndcg value.\n",
    "                hr_mean = np.array(hits).mean()\n",
    "                ndcg_mean = np.array(ndcgs).mean()\n",
    "\n",
    "                print(\"Iter={} [{:.1f} s] HitRatio@{} = {:.4f}, NDCG@{} = {:.4f} [{:.1f} s]\".format(\n",
    "                    iteration, (t2-t1) / 20, topK, hr_mean, topK, ndcg_mean, time.time()-t2))  # 수정된 print 문\n",
    "                t1 = time.time()\n",
    "                iter_loss = 0\n",
    "\n",
    "\n",
    "    def predict(self, u, i):\n",
    "        return np.inner(self.U[u].detach().numpy(), self.V[i].detach().numpy())\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        users, pos_items, neg_items = [], [], []\n",
    "        for i in range(batch_size):  # xrange를 range로 수정\n",
    "            # sample a user\n",
    "            u = np.random.randint(0, self.num_user)\n",
    "            # sample a positive item\n",
    "            i = self.train[u][np.random.randint(0, len(self.train[u]))][0]\n",
    "            # sample a negative item\n",
    "            j = np.random.randint(0, self.num_item)\n",
    "            while j in self.items_of_user[u]:\n",
    "                j = np.random.randint(0, self.num_item)\n",
    "            users.append(u)\n",
    "            pos_items.append(i)\n",
    "            neg_items.append(j)\n",
    "        return (users, pos_items, neg_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457f336d-9ab5-4bbb-a726-fdc3acb8e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRatingFile_HoldKOut(filename, splitter, K):\n",
    "    \"\"\"\n",
    "    Each line of .rating file is: userId(starts from 0), itemId, ratingScore, time\n",
    "    Each element of train is the [[item1, time1], [item2, time2] of the user, sorted by time\n",
    "    Each element of test is the [user, item, time] interaction, sorted by time\n",
    "    \"\"\"\n",
    "    train = []  \n",
    "    test = []\n",
    "    \n",
    "    # load ratings into train.\n",
    "    num_ratings = 0\n",
    "    num_item = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(splitter)\n",
    "            if (len(arr) < 4):\n",
    "                continue\n",
    "            user, item, time = int(arr[0]), int(arr[1]), long(arr[3]) \n",
    "            if (len(train) <= user):\n",
    "                train.append([])\n",
    "            train[user].append([item, time])\n",
    "            num_ratings += 1\n",
    "            num_item = max(item, num_item)\n",
    "            line = f.readline()\n",
    "    num_user = len(train)\n",
    "    num_item = num_item + 1\n",
    "    \n",
    "    # sort ratings of each user by time\n",
    "    def getTime(item):\n",
    "        return item[-1];\n",
    "    for u in range (len(train)):\n",
    "        train[u] = sorted(train[u], key = getTime)\n",
    "    \n",
    "    # split into train/test\n",
    "    for u in range (len(train)):\n",
    "        for k in range(K):\n",
    "            if (len(train[u]) == 0):\n",
    "                break\n",
    "            test.append([u, train[u][-1][0], train[u][-1][1]])\n",
    "            del train[u][-1]    # delete the last element from train\n",
    "            \n",
    "    # sort the test ratings by time\n",
    "    test = sorted(test, key = getTime)\n",
    "    \n",
    "    return train, test, num_user, num_item, num_ratings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Pinterest(Dataset):\n",
    "    def __init__(self, dir, splitter, K):\n",
    "        \"\"\"\n",
    "        Each line of .rating file is: userId(starts from 0), itemId, ratingScore, time\n",
    "        Each element of train is the [[item1, time1], [item2, time2] of the user, sorted by time\n",
    "        Each element of test is the [user, item, time] interaction, sorted by time\n",
    "        \"\"\"\n",
    "\n",
    "        self.train = []\n",
    "        \n",
    "        # load ratings into train.\n",
    "        self.num_ratings = 0\n",
    "        self.num_item = 0\n",
    "        with open(dir+'pos.txt', \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(splitter)\n",
    "                if (len(arr) < 2):\n",
    "                    continue\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                if (len(self.train) <= user):\n",
    "                    self.train.append([])\n",
    "                self.train[user].append([item])\n",
    "                self.num_ratings += 1\n",
    "                self.num_item = max(item, self.num_item)\n",
    "                line = f.readline()\n",
    "        self.num_user = len(self.train)\n",
    "        self.num_item = self.num_item + 1\n",
    "\n",
    "        self.test = []\n",
    "        self.neg = dict()\n",
    "        # load ratings into test.\n",
    "        user = 0\n",
    "        with open(dir+'neg.txt', 'r') as f_neg:\n",
    "            line = f_neg.readline()\n",
    "            while line != None and line != '':\n",
    "                arr = line.split(splitter)\n",
    "                pos = int(arr[0])\n",
    "                self.test.append([user, pos])\n",
    "                self.neg[user] = []\n",
    "                for neg_i in range(len(arr)):\n",
    "                    if arr[neg_i] != '\\n':\n",
    "                        self.neg[user].append(int(arr[neg_i]))\n",
    "\n",
    "                user += 1\n",
    "                line = f_neg.readline()\n",
    "        print(\"#users: %d, #items: %d, #ratings: %d\" %(self.num_user, self.num_item, self.num_ratings))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_user\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = idx\n",
    "        i = self.train[u][np.random.randint(0, len(self.train[u]))]\n",
    "        j = np.random.randint(0, self.num_item)\n",
    "        while j in self.train[u]:\n",
    "            j = np.random.randint(0, self.num_item) \n",
    "        \n",
    "        return (u, i, j)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d536f3-cbda-4e2e-8a46-993c715b78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_K = None\n",
    "\n",
    "def evaluate_model(model, testRatings, K, num_thread):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _K\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _K = K\n",
    "    num_rating = len(testRatings)\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=num_thread)\n",
    "    res = pool.map(eval_one_rating, range(num_rating))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    hits = [r[0] for r in res]\n",
    "    ndcgs = [r[1] for r in res]\n",
    "#    lp.print_stats()\n",
    "    return (hits, ndcgs)\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    hr = ndcg = 0\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    map_item_score = {}\n",
    "\n",
    "    # Get the score of the test item first\n",
    "    maxScore = _model.predict(u, gtItem)\n",
    "\n",
    "    # Early stopping if there are K items larger than maxScore.\n",
    "    countLarger = 0\n",
    "    for i in _model.neg[u]:\n",
    "        early_stop = False\n",
    "        score = _model.predict(u, i)\n",
    "        map_item_score[i] = score\n",
    "\n",
    "        if score > maxScore:\n",
    "            countLarger += 1\n",
    "        if countLarger > _K:\n",
    "            hr = ndcg = 0\n",
    "            early_stop = True\n",
    "            break\n",
    "    if not early_stop:\n",
    "        ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "        hr = getHitRatio(ranklist, gtItem)\n",
    "        ndcg = getNDCG(ranklist, gtItem)\n",
    "\n",
    "    return (hr, ndcg)\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd7ed1e-ea94-4028-885f-eb55e05abc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "#users: 3226, #items: 4998, #ratings: 6618\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pinterest' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m hold_k_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m pinterest \u001b[38;5;241m=\u001b[39m Pinterest(dataset, splitter, hold_k_out)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mpinterest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# MFbpr parameters\u001b[39;00m\n\u001b[0;32m     21\u001b[0m factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pinterest' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "def parse_args():\n",
    "    args = argparse.Namespace()\n",
    "    args.batch_size = 32\n",
    "    args.learning_rate = 0.0003\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    # Load data\n",
    "    dataset = \"./data/\"\n",
    "    splitter = \" \"\n",
    "    hold_k_out = 1\n",
    "    pinterest = Pinterest(dataset, splitter, hold_k_out)\n",
    "\n",
    "    pinterest\n",
    "    \n",
    "    # MFbpr parameters\n",
    "    factors = 64\n",
    "    learning_rate = args.learning_rate\n",
    "    reg = 0.01\n",
    "    init_mean = 0\n",
    "    init_stdev = 0.01\n",
    "    maxIter = 10000\n",
    "    batch_size = args.batch_size\n",
    "    num_thread = mp.cpu_count()\n",
    "    print(\"#factors: %d, lr: %f, reg: %f, batch_size: %d\" % (factors, learning_rate, reg, batch_size))\n",
    "    \n",
    "    # Run model\n",
    "    bpr = MFbpr(pinterest,\n",
    "                factors, learning_rate, reg, init_mean, init_stdev)\n",
    "    bpr.build_model(maxIter, num_thread, batch_size=batch_size)\n",
    "\n",
    "    # save model\n",
    "    np.save(\"out/u\"+str(learning_rate)+\".npy\", bpr.U.detach().numpy())\n",
    "    np.save(\"out/v\"+str(learning_rate)+\".npy\", bpr.V.detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
