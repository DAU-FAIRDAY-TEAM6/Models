{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fNaE73RP23kP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "import multiprocessing as mp\n",
        "import argparse\n",
        "\n",
        "import math\n",
        "import heapq # for retrieval topK\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # tqdm 라이브러리 임포트\n",
        "from tqdm.auto import trange\n",
        "import random\n",
        "from multiprocessing import Pool\n",
        "import _multiprocessing\n",
        "import pickle\n",
        "from scipy.spatial.distance import pdist, squareform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5klGMkPE28sE"
      },
      "outputs": [],
      "source": [
        "def get_data(path = \"/content/drive/MyDrive/학교/졸업작품/\"):\n",
        "\n",
        "    business_info_file = path + 'business_info.csv' # 필라델피아 가게 정보 business_id, latitude, longitude, city, idx\n",
        "\n",
        "    business_location = []\n",
        "    with open(business_info_file, 'r', newline='') as business_file:\n",
        "        csv_reader = csv.reader(business_file)\n",
        "        next(csv_reader)  # 헤더 행 건너뛰기\n",
        "        for row in csv_reader:\n",
        "            _, latitude, longitude, _, _ = row[0], row[1], row[2], row[3].lower(), row[4] #city : 소문자로 받음\n",
        "\n",
        "            business_location.append([latitude, longitude])\n",
        "    business_location = np.array(business_location, dtype=float)\n",
        "\n",
        "    input_file = path + \"reviews.txt\"\n",
        "    data = []\n",
        "    with open(input_file, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            data.append(row)\n",
        "\n",
        "    if not data:  # 데이터가 비어있는 경우 처리\n",
        "        return\n",
        "\n",
        "    user_history_list = []\n",
        "    user_reviews_list = []\n",
        "    user_ratings_list = []\n",
        "    user_review_emb_list = []\n",
        "    user_frequency_list = []\n",
        "\n",
        "    tmp_reviews = []\n",
        "    tmp_ratings = []\n",
        "    tmp_business_id = []\n",
        "    tmp_review_emb_list = []\n",
        "    tmp_frequency = {}\n",
        "\n",
        "    before_user_id = data[0][0]  # 첫 번째 사용자 ID로 초기화\n",
        "    for idx, i in enumerate(data):\n",
        "        user_id, business_id, rating, review = i\n",
        "        check_rating = float(rating) > 3.0\n",
        "        business_id = int(business_id)\n",
        "        if user_id == before_user_id:\n",
        "            tmp_business_id.append(business_id)\n",
        "            tmp_ratings.append(float(rating))\n",
        "            tmp_reviews.append(review)\n",
        "            tmp_review_emb_list.append((idx,check_rating))\n",
        "            if business_id in tmp_frequency:\n",
        "                tmp_frequency[business_id] += 1\n",
        "            else:\n",
        "                tmp_frequency[business_id] = 1\n",
        "        else:\n",
        "            if len(tmp_business_id) >= 10:  # 방문 횟수가 10회가 넘는 유저만 append\n",
        "                user_history_list.append(tmp_business_id)\n",
        "                user_ratings_list.append(tmp_ratings)\n",
        "                user_reviews_list.append(tmp_reviews)\n",
        "                user_review_emb_list.append(tmp_review_emb_list)\n",
        "                user_frequency_list.append(tmp_frequency)\n",
        "            tmp_business_id = [int(business_id)]\n",
        "            tmp_ratings = [float(rating)]\n",
        "            tmp_reviews = [review]\n",
        "            tmp_review_emb_list = [(idx, check_rating)]\n",
        "            tmp_frequency = {business_id: 1}\n",
        "\n",
        "            before_user_id = user_id  # 현재 사용자 ID로 업데이트\n",
        "\n",
        "    # 마지막 사용자 처리\n",
        "    if len(tmp_business_id) >= 10:\n",
        "        user_history_list.append(tmp_business_id)\n",
        "        user_ratings_list.append(tmp_ratings)\n",
        "        user_reviews_list.append(tmp_reviews)\n",
        "        user_review_emb_list.append(tmp_review_emb_list)\n",
        "        user_frequency_list.append(tmp_frequency)\n",
        "    print(len(user_history_list), len(user_ratings_list), len(user_reviews_list), len(user_review_emb_list))\n",
        "\n",
        "    # POI가 가진 리뷰 임베딩을 획득하기 위해\n",
        "    # history_list를 기준으로 POI에 방문한 사람들 list 생성\n",
        "    poi_visited_list = []\n",
        "    for user,history in enumerate(user_history_list):\n",
        "        for idx, poi in enumerate(history):\n",
        "            poi_visited_list.append([int(user), int(poi), float(user_ratings_list[user][idx]), user_reviews_list[user][idx], user_review_emb_list[user][idx]])\n",
        "\n",
        "    poi_visited_list.sort(key = lambda x:x[1]) # poi 번호 순으로 정렬\n",
        "\n",
        "    item_history_list = []\n",
        "    item_reviews_list = []\n",
        "    item_ratings_list = []\n",
        "    item_review_emb_list = []\n",
        "\n",
        "\n",
        "    tmp_reviews = []\n",
        "    tmp_ratings = []\n",
        "    tmp_user_id = []\n",
        "    tmp_review_emb = []\n",
        "    before_poi_id = poi_visited_list[0][1]  # 첫 번째 POI ID로 초기화\n",
        "\n",
        "    for idx, i in enumerate(poi_visited_list):\n",
        "        user_id, business_id, rating, review, review_emb = i[0], i[1], i[2], i[3], i[4]\n",
        "        if business_id == before_poi_id: # 이전 POI Id와 동일하다면\n",
        "            tmp_user_id.append(user_id)\n",
        "            tmp_ratings.append(rating)\n",
        "            tmp_reviews.append(review)\n",
        "            tmp_review_emb.append(review_emb)\n",
        "        else: # 이전 POI ID와 다른 POI라면\n",
        "            #print(business_id)\n",
        "            # 이전 POI 정보 안에 있던거 다 추가하고\n",
        "            item_history_list.append(tmp_user_id)\n",
        "            item_ratings_list.append(tmp_ratings)\n",
        "            item_reviews_list.append(tmp_reviews)\n",
        "            item_review_emb_list.append(tmp_review_emb)\n",
        "\n",
        "            if int(business_id) - int(before_poi_id) > 1:\n",
        "                for _ in range(int(business_id) - int(before_poi_id) - 1):\n",
        "                    #print(f\"방문 기록이 없는 POI는 PASS\")\n",
        "                    item_history_list.append([])\n",
        "                    item_ratings_list.append([])\n",
        "                    item_reviews_list.append([])\n",
        "                    item_review_emb_list.append([])\n",
        "\n",
        "            tmp_user_id = [user_id]\n",
        "            tmp_ratings = [rating]\n",
        "            tmp_reviews = [review]\n",
        "            tmp_review_emb = [review_emb]\n",
        "\n",
        "            before_poi_id = business_id  # 현재 사용자 ID로 업데이트\n",
        "\n",
        "    item_history_list.append(tmp_business_id)\n",
        "    item_ratings_list.append(tmp_ratings)\n",
        "    item_reviews_list.append(tmp_reviews)\n",
        "    item_review_emb_list.append(tmp_review_emb)\n",
        "\n",
        "    print(len(item_history_list), len(item_ratings_list), len(item_reviews_list), len(item_review_emb_list))\n",
        "\n",
        "    user_review_embs = []\n",
        "    item_review_embs = []\n",
        "\n",
        "    embedding_file = path + 'embeddings.npy'\n",
        "    embeddings = np.load(embedding_file, mmap_mode='r')\n",
        "\n",
        "    user_review_embs = []\n",
        "    for poi, embeds in enumerate(user_review_emb_list):\n",
        "        if len(embeds)>0: # 비어있지 않으면\n",
        "            # 기존\n",
        "            new_array = np.array([embeddings[idx] for idx, check_rating in embeds])\n",
        "            new_array = np.mean(new_array, axis = 0)\n",
        "\n",
        "            # # 변경\n",
        "            # temp_list = []\n",
        "            # for idx, check_rating in embeds:\n",
        "            #     if check_rating:\n",
        "            #         temp_list.append(embeddings[idx])\n",
        "            # if len(temp_list):\n",
        "            #     new_array = np.array(temp_list)\n",
        "            #     new_array = np.mean(new_array, axis = 0)\n",
        "            # else:\n",
        "            #     new_array = np.zeros(768, dtype=np.float32)\n",
        "        else:\n",
        "            new_array = np.zeros(768, dtype=np.float32)\n",
        "        user_review_embs.append(new_array)\n",
        "\n",
        "    item_review_embs = []\n",
        "    for poi, embeds in enumerate(item_review_emb_list):\n",
        "        if len(embeds)>0: # 비어있지 않으면\n",
        "            new_array = np.array([embeddings[idx] for idx, check_rating in embeds])\n",
        "            new_array = np.mean(new_array, axis = 0)\n",
        "        else:\n",
        "            new_array = np.zeros(768, dtype=np.float32)\n",
        "\n",
        "        item_review_embs.append(new_array.tolist())\n",
        "    \n",
        "    return user_history_list, user_ratings_list, user_reviews_list, user_review_embs, item_history_list, item_ratings_list, item_reviews_list, item_review_embs, business_location, user_frequency_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "m6qRtumj3FXu"
      },
      "outputs": [],
      "source": [
        "class Yelp(Dataset):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Yelp 데이터셋을 로드하고 학습 데이터와 테스트 데이터를 생성합니다.\n",
        "\n",
        "        Args:\n",
        "            dir (str): 데이터 파일이 있는 디렉토리 경로.\n",
        "            splitter (str): 파일에서 열을 구분하는 구분자.\n",
        "            K (int): K 값, 즉 각 사용자마다 테스트에 사용되는 상호작용의 수.\n",
        "        \"\"\"\n",
        "        path = 'yelp/'\n",
        "        user_history_list, _, _, user_review_embeds ,_,_,_,poi_review_embeds, business_location, user_frequency_list = get_data(path)\n",
        "        self.norm_distances = self.normalize_distances(self.calculate_distances(business_location))\n",
        "        self.norm_frequencys = self.normalize_frequency(self.calculate_frequency(user_frequency_list))\n",
        "\n",
        "        self.train = []\n",
        "        self.test = []\n",
        "        self.poi_review_embeds = torch.tensor(poi_review_embeds).to(DEVICE)\n",
        "        self.user_review_embeds = torch.tensor(user_review_embeds).to(DEVICE)\n",
        "        self.num_user = len(user_history_list)\n",
        "        self.num_item = len(poi_review_embeds) # 14585\n",
        "\n",
        "        items = [i for i in range(self.num_item)]\n",
        "        self.neg = dict()\n",
        "\n",
        "        random.seed(30)\n",
        "        for u, hist in enumerate(user_history_list):\n",
        "            random.shuffle(hist)\n",
        "            self.train.append(hist[:int(len(hist) * 0.7)])\n",
        "            self.test.append(hist[int(len(hist) * 0.7) :])\n",
        "\n",
        "            u_negs = set(items) - set(hist)\n",
        "            self.neg[u] = list(u_negs) # ng dataset 생성\n",
        "\n",
        "        # self.test_for_eval = []\n",
        "        # for u,hist in enumerate(self.test):\n",
        "        #     for i in hist:\n",
        "        #         self.test_for_eval.append([u,i])\n",
        "\n",
        "        self.index_map = []\n",
        "        for u, user_items in enumerate(self.train):\n",
        "            for i in user_items:\n",
        "                self.index_map.append((u, i))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        데이터셋의 사용자 수를 반환합니다.\n",
        "        \"\"\"\n",
        "        #return self.num_user\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        데이터셋에서 하나의 샘플을 가져옵니다.\n",
        "\n",
        "        Args:\n",
        "            idx (int): 데이터셋 내의 인덱스.\n",
        "\n",
        "        Returns:\n",
        "            u: 사용자 ID.\n",
        "            i: 긍정적인 아이템 ID.\n",
        "            j: 부정적인 아이템 ID.\n",
        "        \"\"\"\n",
        "        # u = idx\n",
        "        # # 사용자별로 하나의 긍정적인 상호작용 선택\n",
        "        # i = self.train[u][np.random.randint(0, len(self.train[u]))]\n",
        "        # # 부정적인 상호작용 무작위 선택\n",
        "        # j = self.neg[u][np.random.randint(0, len(self.neg[u]))]\n",
        "        # #j = random.sample(self.neg[u], 4)\n",
        "        # return (u, i, j)\n",
        "\n",
        "        u, i = self.index_map[idx]\n",
        "        # 부정적인 아이템 무작위 선택\n",
        "        j = self.neg[u][np.random.randint(0, len(self.neg[u]))]\n",
        "        return (u, i, j)\n",
        "\n",
        "    def haversine(self, lat1, lon1, lat2, lon2):\n",
        "        R = 6371\n",
        "        dlat = np.radians(lat2 - lat1)\n",
        "        dlon = np.radians(lon1 - lon2)  # Note the change here\n",
        "        a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2\n",
        "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "        d = R * c\n",
        "        return d\n",
        "\n",
        "    def calculate_distances(self, poi_data):\n",
        "        t = time.time()\n",
        "        distances = squareform(pdist(poi_data, lambda u, v: self.haversine(u[0], u[1], v[0], v[1])))\n",
        "        print(f\"calculate_distance time : {int(time.time()-t)}\")\n",
        "        return distances\n",
        "\n",
        "    def normalize_distances(self, distances):\n",
        "        min_d = np.min(distances)\n",
        "        max_d = np.max(distances)\n",
        "        norm_distances = 0.5 * (distances - min_d) / (max_d - min_d) + 0.5\n",
        "        return norm_distances\n",
        "    \n",
        "    def calculate_frequency(self, frequency_list):\n",
        "        frequency_diff = {}\n",
        "        for u, freq_dict in enumerate(frequency_list):\n",
        "            pois = list(freq_dict.keys())\n",
        "            for i in pois:\n",
        "                for j in pois:\n",
        "                    if i != j:\n",
        "                        fui = freq_dict.get(i, 0)  # POI i의 빈도수\n",
        "                        fuj = freq_dict.get(j, 0)  # POI j의 빈도수\n",
        "                        fuij = fui - fuj  # 빈도수 차이\n",
        "                        business_pair = (i, j)\n",
        "                        frequency_diff[business_pair] = fuij\n",
        "        return frequency_diff\n",
        "\n",
        "    def normalize_frequency(self, frequency_diff):\n",
        "        min_freq_diff = min(frequency_diff.values())  # 최소 빈도수 차이\n",
        "        max_freq_diff = max(frequency_diff.values())  # 최대 빈도수 차이\n",
        "        norm_frequencys = {}\n",
        "        for key, fuij in frequency_diff.items():\n",
        "            norm_fuij = 0.5 * ((fuij - min_freq_diff) / (max_freq_diff - min_freq_diff)) + 0.5\n",
        "            norm_frequencys[key] = norm_fuij\n",
        "        return norm_frequencys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Jk9LEkZ6UD90"
      },
      "outputs": [],
      "source": [
        "class MFbpr(nn.Module):\n",
        "    '''\n",
        "    MF 모델에 대한 BPR 학습\n",
        "    '''\n",
        "    def __init__(self, dataset, factors, learning_rate, reg, init_mean, init_stdev):\n",
        "        '''\n",
        "        생성자\n",
        "        Args:\n",
        "            dataset: 데이터셋 객체로, 학습 및 테스트 데이터를 포함합니다.\n",
        "            factors (int): 잠재 요인의 수.\n",
        "            learning_rate (float): 최적화에 사용되는 학습률.\n",
        "            reg (float): 정규화 강도.\n",
        "            init_mean (float): 초기화에 사용되는 정규 분포의 평균.\n",
        "            init_stdev (float): 초기화에 사용되는 정규 분포의 표준 편차.\n",
        "        '''\n",
        "        super(MFbpr, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.train_data = dataset.train\n",
        "        self.test_data = dataset.test\n",
        "        self.num_user = dataset.num_user\n",
        "        self.num_item = dataset.num_item\n",
        "        self.neg = dataset.neg\n",
        "        self.factors = factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg = reg\n",
        "        self.init_mean = init_mean\n",
        "        self.init_stdev = init_stdev\n",
        "\n",
        "\n",
        "        # 사용자와 아이템의 잠재 요인을 초기화합니다.\n",
        "        self.embed_user = torch.normal(mean=self.init_mean * torch.ones(self.num_user, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "        self.embed_item = torch.normal(mean=self.init_mean * torch.ones(self.num_item, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "\n",
        "        # Adam optimizer를 초기화합니다.\n",
        "        self.mf_optim = optim.Adam([self.embed_user, self.embed_item], lr=self.learning_rate)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        '''\n",
        "        MF-BPR 모델의 forward pass입니다.\n",
        "        Args:\n",
        "            u: 사용자 ID.\n",
        "            i: 긍정적인 아이템 ID.\n",
        "            j: 부정적인 아이템 ID.\n",
        "        Returns:\n",
        "            y_ui: 사용자와 긍정적인 아이템 간의 예측 점수.\n",
        "            y_uj: 사용자와 부정적인 아이템 간의 예측 점수.\n",
        "            loss: BPR 손실.\n",
        "        '''\n",
        "        # 사용자와 긍정적인 아이템 간의 예측 점수 계산\n",
        "        y_ui = (self.embed_user[u] * self.embed_item[i]).sum(dim=-1)\n",
        "        # 사용자와 부정적인 아이템 간의 예측 점수 계산\n",
        "        y_uj = (self.embed_user[u] * self.embed_item[j]).sum(dim=-1)\n",
        "        # 정규화 항 계산\n",
        "        regularizer = self.reg * (torch.sum(self.embed_user[u] ** 2) + torch.sum(self.embed_item[i] ** 2) + torch.sum(self.embed_item[j] ** 2))\n",
        "        # BPR 손실 계산\n",
        "        loss = regularizer - torch.sum(torch.log(torch.sigmoid(y_ui - y_uj)))\n",
        "        return y_ui, y_uj, loss\n",
        "\n",
        "    def build_model(self, epoch=30, batch_size=32, topK = 10):\n",
        "        '''\n",
        "        MF-BPR 모델을 구축하고 학습합니다.\n",
        "        Args:\n",
        "            epoch (int): 학습의 최대 반복 횟수.\n",
        "            num_thread (int): 병렬 실행을 위한 스레드 수.\n",
        "            batch_size (int): 학습용 배치 크기.\n",
        "        '''\n",
        "        data_loader = DataLoader(self.dataset, batch_size=batch_size)\n",
        "\n",
        "        print(\"Training MF-BPR with: learning_rate=%.4f, regularization=%.7f, factors=%d, #epoch=%d, batch_size=%d.\"\n",
        "               % (self.learning_rate, self.reg, self.factors, epoch, batch_size))\n",
        "        t1 = time.time()\n",
        "\n",
        "        max_hit, max_precision, max_recall, max_recall_epoch, max_precision_epoch, max_hit_epoch = 0,0,0,0,0,0\n",
        "        for epoc in range(epoch):\n",
        "            iter_loss = 0\n",
        "            for s, (users, items_pos, items_neg) in enumerate(data_loader):\n",
        "                # 기울기 초기화\n",
        "                self.mf_optim.zero_grad()\n",
        "                # Forward pass를 통해 예측과 손실 계산\n",
        "                y_ui, y_uj, loss = self.forward(users, items_pos, items_neg)\n",
        "                iter_loss += loss\n",
        "                # Backward pass 및 파라미터 업데이트\n",
        "                loss.backward()\n",
        "                self.mf_optim.step()\n",
        "            t2 = time.time()\n",
        "\n",
        "            # 성능 측정 함수를 통해 HitRatio 및 NDCG를 계산\n",
        "\n",
        "            hits, recall, precision = self.evaluate_model(self.test_data, topK)\n",
        "\n",
        "            print(f\"epoch={epoc}, loss = {iter_loss}[{int(t2-t1)}s] HitRatio@{topK} = {hits}, RECAll@{topK} = {recall}, PRECISION@{topK} = {precision} [{int(time.time()-t2)}s]\")\n",
        "            t1 = time.time()\n",
        "            if precision > max_precision:\n",
        "                max_precision = precision\n",
        "                max_precision_epoch = epoc\n",
        "            if recall > max_recall:\n",
        "                max_recall = recall\n",
        "                max_recall_epoch = epoc\n",
        "            if hits > max_hit:\n",
        "                max_hit = hits\n",
        "                max_hit_epoch = epoc\n",
        "            t1 = time.time()\n",
        "\n",
        "        #save_perform(reg, batch_size, latent_factors, text_factors, epoc, learning_rate, max_hit, max_hit_epoch, max_recall, max_recall_epoch, max_precision, max_precision_epoch)\n",
        "\n",
        "\n",
        "    def evaluate_model(self, test, K):\n",
        "        \"\"\"\n",
        "        Top-K 추천의 성능(Hit_Ratio, NDCG)을 평가합니다.\n",
        "        반환값: 각 테스트 상호작용의 점수.\n",
        "        \"\"\"\n",
        "        score_matrix = torch.mm(self.embed_user, self.embed_item.t())\n",
        "        top_scores, top_indicies = torch.topk(score_matrix, K, dim=1)\n",
        "\n",
        "        hits = 0\n",
        "        sum_recall = 0\n",
        "        sum_precision = 0\n",
        "        for u,hist in enumerate(test):\n",
        "            set_topk = set(i.item() for i in (top_indicies[u]))\n",
        "            set_hist = set(hist)\n",
        "\n",
        "            if set_hist & set_topk:\n",
        "                hits += 1\n",
        "            sum_precision += len(set_hist & set_topk) / len(set_topk)\n",
        "            sum_recall += len(set_hist & set_topk) / len(set_hist)\n",
        "\n",
        "        return hits / len(test), sum_recall / len(test), sum_precision / len(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Mna58Raj68ET"
      },
      "outputs": [],
      "source": [
        "class DistBPR(nn.Module):\n",
        "    '''\n",
        "    거리(Distance)에 대한 BPR 학습\n",
        "    '''\n",
        "    def __init__(self, dataset, factors, learning_rate, reg, init_mean, init_stdev):\n",
        "        '''\n",
        "        생성자\n",
        "        Args:\n",
        "            dataset: 데이터셋 객체로, 학습 및 테스트 데이터를 포함합니다.\n",
        "            factors (int): 잠재 요인의 수.\n",
        "            learning_rate (float): 최적화에 사용되는 학습률.\n",
        "            reg (float): 정규화 강도.\n",
        "            init_mean (float): 초기화에 사용되는 정규 분포의 평균.\n",
        "            init_stdev (float): 초기화에 사용되는 정규 분포의 표준 편차.\n",
        "        '''\n",
        "        super(DistBPR, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.train_data = dataset.train\n",
        "        self.norm_distances = dataset.norm_distances\n",
        "        self.test_data = dataset.test\n",
        "        self.num_user = dataset.num_user\n",
        "        self.num_item = dataset.num_item\n",
        "        self.neg = dataset.neg\n",
        "        self.factors = factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg = reg\n",
        "        self.init_mean = init_mean\n",
        "        self.init_stdev = init_stdev\n",
        "\n",
        "\n",
        "        # 사용자와 아이템의 잠재 요인을 초기화합니다.\n",
        "        self.embed_user = torch.normal(mean=self.init_mean * torch.ones(self.num_user, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "        self.embed_item = torch.normal(mean=self.init_mean * torch.ones(self.num_item, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "\n",
        "        # Adam optimizer를 초기화합니다.\n",
        "        self.mf_optim = optim.Adam([self.embed_user, self.embed_item], lr=self.learning_rate)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        '''\n",
        "        DBPR 모델의 forward pass입니다.\n",
        "        Args:\n",
        "            u: 사용자 ID.\n",
        "            i: 긍정적인 아이템 ID.\n",
        "            j: 부정적인 아이템 ID.\n",
        "        Returns:\n",
        "            y_ui: 사용자와 긍정적인 아이템 간의 예측 점수.\n",
        "            y_uj: 사용자와 부정적인 아이템 간의 예측 점수.\n",
        "            loss: BPR 손실.\n",
        "        '''\n",
        "        # 사용자와 긍정적인 아이템 간의 예측 점수 계산\n",
        "        y_ui = (self.embed_user[u] * self.embed_item[i]).sum(dim=-1)\n",
        "        # 사용자와 부정적인 아이템 간의 예측 점수 계산\n",
        "        y_uj = (self.embed_user[u] * self.embed_item[j]).sum(dim=-1)\n",
        "        # i와 j 정규화 거리\n",
        "        distance_ij = torch.tensor(self.norm_distances[i, j]).to(DEVICE)\n",
        "        # 정규화 항 계산\n",
        "        regularizer = self.reg * (torch.sum(self.embed_user[u] ** 2) + torch.sum(self.embed_item[i] ** 2) + torch.sum(self.embed_item[j] ** 2))\n",
        "        # BPR 손실 계산\n",
        "        loss = regularizer - torch.sum(torch.log(torch.sigmoid(distance_ij*(y_ui - y_uj))))\n",
        "        return y_ui, y_uj, loss\n",
        "\n",
        "    def build_model(self, epoch=30, batch_size=32, topK = 10):\n",
        "        '''\n",
        "        DBPR 모델을 구축하고 학습합니다.\n",
        "        Args:\n",
        "            epoch (int): 학습의 최대 반복 횟수.\n",
        "            num_thread (int): 병렬 실행을 위한 스레드 수.\n",
        "            batch_size (int): 학습용 배치 크기.\n",
        "        '''\n",
        "        data_loader = DataLoader(self.dataset, batch_size=batch_size)\n",
        "\n",
        "        print(\"Training MF-BPR with: learning_rate=%.4f, regularization=%.7f, factors=%d, #epoch=%d, batch_size=%d.\"\n",
        "               % (self.learning_rate, self.reg, self.factors, epoch, batch_size))\n",
        "        t1 = time.time()\n",
        "\n",
        "        max_hit, max_precision, max_recall, max_recall_epoch, max_precision_epoch, max_hit_epoch = 0,0,0,0,0,0\n",
        "        for epoc in range(epoch):\n",
        "            iter_loss = 0\n",
        "            for s, (users, items_pos, items_neg) in enumerate(data_loader):\n",
        "                # 기울기 초기화\n",
        "                self.mf_optim.zero_grad()\n",
        "                # Forward pass를 통해 예측과 손실 계산\n",
        "                y_ui, y_uj, loss = self.forward(users, items_pos, items_neg)\n",
        "                iter_loss += loss\n",
        "                # Backward pass 및 파라미터 업데이트\n",
        "                loss.backward()\n",
        "                self.mf_optim.step()\n",
        "            t2 = time.time()\n",
        "\n",
        "            # 성능 측정 함수를 통해 HitRatio 및 NDCG를 계산\n",
        "\n",
        "            hits, recall, precision = self.evaluate_model(self.test_data, topK)\n",
        "\n",
        "            print(f\"epoch={epoc}, loss = {iter_loss}[{int(t2-t1)}s] HitRatio@{topK} = {hits}, RECAll@{topK} = {recall}, PRECISION@{topK} = {precision} [{int(time.time()-t2)}s]\")\n",
        "            t1 = time.time()\n",
        "            if precision > max_precision:\n",
        "                max_precision = precision\n",
        "                max_precision_epoch = epoc\n",
        "            if recall > max_recall:\n",
        "                max_recall = recall\n",
        "                max_recall_epoch = epoc\n",
        "            if hits > max_hit:\n",
        "                max_hit = hits\n",
        "                max_hit_epoch = epoc\n",
        "            t1 = time.time()\n",
        "\n",
        "        #save_perform(reg, batch_size, latent_factors, text_factors, epoc, learning_rate, max_hit, max_hit_epoch, max_recall, max_recall_epoch, max_precision, max_precision_epoch)\n",
        "\n",
        "\n",
        "    def evaluate_model(self, test, K):\n",
        "        \"\"\"\n",
        "        Top-K 추천의 성능(Hit_Ratio, NDCG)을 평가합니다.\n",
        "        반환값: 각 테스트 상호작용의 점수.\n",
        "        \"\"\"\n",
        "        score_matrix = torch.mm(self.embed_user, self.embed_item.t())\n",
        "        top_scores, top_indicies = torch.topk(score_matrix, K, dim=1)\n",
        "\n",
        "        hits = 0\n",
        "        sum_recall = 0\n",
        "        sum_precision = 0\n",
        "        for u,hist in enumerate(test):\n",
        "            set_topk = set(i.item() for i in (top_indicies[u]))\n",
        "            set_hist = set(hist)\n",
        "\n",
        "            if set_hist & set_topk:\n",
        "                hits += 1\n",
        "            sum_precision += len(set_hist & set_topk) / len(set_topk)\n",
        "            sum_recall += len(set_hist & set_topk) / len(set_hist)\n",
        "\n",
        "        return hits / len(test), sum_recall / len(test), sum_precision / len(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FreqBPR(nn.Module):\n",
        "    '''\n",
        "    거리(frequency)에 대한 BPR 학습\n",
        "    '''\n",
        "    def __init__(self, dataset, factors, learning_rate, reg, init_mean, init_stdev):\n",
        "        '''\n",
        "        생성자\n",
        "        Args:\n",
        "            dataset: 데이터셋 객체로, 학습 및 테스트 데이터를 포함합니다.\n",
        "            factors (int): 잠재 요인의 수.\n",
        "            learning_rate (float): 최적화에 사용되는 학습률.\n",
        "            reg (float): 정규화 강도.\n",
        "            init_mean (float): 초기화에 사용되는 정규 분포의 평균.\n",
        "            init_stdev (float): 초기화에 사용되는 정규 분포의 표준 편차.\n",
        "        '''\n",
        "        super(FreqBPR, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.train_data = dataset.train\n",
        "        self.test_data = dataset.test\n",
        "        self.norm_frequencys = dataset.norm_frequencys\n",
        "        self.num_user = dataset.num_user\n",
        "        self.num_item = dataset.num_item\n",
        "        self.neg = dataset.neg\n",
        "        self.factors = factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg = reg\n",
        "        self.init_mean = init_mean\n",
        "        self.init_stdev = init_stdev\n",
        "\n",
        "\n",
        "        # 사용자와 아이템의 잠재 요인을 초기화합니다.\n",
        "        self.embed_user = torch.normal(mean=self.init_mean * torch.ones(self.num_user, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "        self.embed_item = torch.normal(mean=self.init_mean * torch.ones(self.num_item, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "\n",
        "        # Adam optimizer를 초기화합니다.\n",
        "        self.mf_optim = optim.Adam([self.embed_user, self.embed_item], lr=self.learning_rate)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        '''\n",
        "        DBPR 모델의 forward pass입니다.\n",
        "        Args:\n",
        "            u: 사용자 ID.\n",
        "            i: 긍정적인 아이템 ID.\n",
        "            j: 부정적인 아이템 ID.\n",
        "        Returns:\n",
        "            y_ui: 사용자와 긍정적인 아이템 간의 예측 점수.\n",
        "            y_uj: 사용자와 부정적인 아이템 간의 예측 점수.\n",
        "            loss: BPR 손실.\n",
        "        '''\n",
        "        # 사용자와 긍정적인 아이템 간의 예측 점수 계산\n",
        "        y_ui = (self.embed_user[u] * self.embed_item[i]).sum(dim=-1)\n",
        "        # 사용자와 부정적인 아이템 간의 예측 점수 계산\n",
        "        y_uj = (self.embed_user[u] * self.embed_item[j]).sum(dim=-1)\n",
        "        # i와 j 정규화 거리\n",
        "        frequency_ij = torch.tensor([self.norm_frequencys.get((int(i[idx]), int(j[idx])), 0.5) for idx in range(len(i))]).to(DEVICE)\n",
        "\n",
        "        # 정규화 항 계산\n",
        "        regularizer = self.reg * (torch.sum(self.embed_user[u] ** 2) + torch.sum(self.embed_item[i] ** 2) + torch.sum(self.embed_item[j] ** 2))\n",
        "        # BPR 손실 계산\n",
        "        loss = regularizer - torch.sum(torch.log(torch.sigmoid(frequency_ij*(y_ui - y_uj))))\n",
        "        return y_ui, y_uj, loss\n",
        "\n",
        "    def build_model(self, epoch=30, batch_size=32, topK = 10):\n",
        "        '''\n",
        "        DBPR 모델을 구축하고 학습합니다.\n",
        "        Args:\n",
        "            epoch (int): 학습의 최대 반복 횟수.\n",
        "            num_thread (int): 병렬 실행을 위한 스레드 수.\n",
        "            batch_size (int): 학습용 배치 크기.\n",
        "        '''\n",
        "        data_loader = DataLoader(self.dataset, batch_size=batch_size)\n",
        "\n",
        "        print(\"Training MF-BPR with: learning_rate=%.4f, regularization=%.7f, factors=%d, #epoch=%d, batch_size=%d.\"\n",
        "               % (self.learning_rate, self.reg, self.factors, epoch, batch_size))\n",
        "        t1 = time.time()\n",
        "\n",
        "        max_hit, max_precision, max_recall, max_recall_epoch, max_precision_epoch, max_hit_epoch = 0,0,0,0,0,0\n",
        "        for epoc in range(epoch):\n",
        "            iter_loss = 0\n",
        "            for s, (users, items_pos, items_neg) in enumerate(data_loader):\n",
        "                # 기울기 초기화\n",
        "                self.mf_optim.zero_grad()\n",
        "                # Forward pass를 통해 예측과 손실 계산\n",
        "                y_ui, y_uj, loss = self.forward(users, items_pos, items_neg)\n",
        "                iter_loss += loss\n",
        "                # Backward pass 및 파라미터 업데이트\n",
        "                loss.backward()\n",
        "                self.mf_optim.step()\n",
        "            t2 = time.time()\n",
        "\n",
        "            # 성능 측정 함수를 통해 HitRatio 및 NDCG를 계산\n",
        "\n",
        "            hits, recall, precision = self.evaluate_model(self.test_data, topK)\n",
        "\n",
        "            print(f\"epoch={epoc}, loss = {iter_loss}[{int(t2-t1)}s] HitRatio@{topK} = {hits}, RECAll@{topK} = {recall}, PRECISION@{topK} = {precision} [{int(time.time()-t2)}s]\")\n",
        "            t1 = time.time()\n",
        "            if precision > max_precision:\n",
        "                max_precision = precision\n",
        "                max_precision_epoch = epoc\n",
        "            if recall > max_recall:\n",
        "                max_recall = recall\n",
        "                max_recall_epoch = epoc\n",
        "            if hits > max_hit:\n",
        "                max_hit = hits\n",
        "                max_hit_epoch = epoc\n",
        "            t1 = time.time()\n",
        "\n",
        "        #save_perform(reg, batch_size, latent_factors, text_factors, epoc, learning_rate, max_hit, max_hit_epoch, max_recall, max_recall_epoch, max_precision, max_precision_epoch)\n",
        "\n",
        "\n",
        "    def evaluate_model(self, test, K):\n",
        "        \"\"\"\n",
        "        Top-K 추천의 성능(Hit_Ratio, NDCG)을 평가합니다.\n",
        "        반환값: 각 테스트 상호작용의 점수.\n",
        "        \"\"\"\n",
        "        score_matrix = torch.mm(self.embed_user, self.embed_item.t())\n",
        "        top_scores, top_indicies = torch.topk(score_matrix, K, dim=1)\n",
        "\n",
        "        hits = 0\n",
        "        sum_recall = 0\n",
        "        sum_precision = 0\n",
        "        for u,hist in enumerate(test):\n",
        "            set_topk = set(i.item() for i in (top_indicies[u]))\n",
        "            set_hist = set(hist)\n",
        "\n",
        "            if set_hist & set_topk:\n",
        "                hits += 1\n",
        "            sum_precision += len(set_hist & set_topk) / len(set_topk)\n",
        "            sum_recall += len(set_hist & set_topk) / len(set_hist)\n",
        "\n",
        "        return hits / len(test), sum_recall / len(test), sum_precision / len(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "iU55gpzKbQVq"
      },
      "outputs": [],
      "source": [
        "class TextBPR(nn.Module):\n",
        "    '''\n",
        "    MF 모델에 대한 BPR 학습\n",
        "    '''\n",
        "    def __init__(self, dataset, factors, text_factors, learning_rate, reg, init_mean, init_stdev, alpha):\n",
        "        '''\n",
        "        생성자\n",
        "        Args:\n",
        "            dataset: 데이터셋 객체로, 학습 및 테스트 데이터를 포함합니다.\n",
        "            factors (int): 잠재 요인의 수.\n",
        "            learning_rate (float): 최적화에 사용되는 학습률.\n",
        "            reg (float): 정규화 강도.\n",
        "            init_mean (float): 초기화에 사용되는 정규 분포의 평균.\n",
        "            init_stdev (float): 초기화에 사용되는 정규 분포의 표준 편차.\n",
        "        '''\n",
        "        super(TextBPR, self).__init__()\n",
        "        self.dataset = dataset\n",
        "        self.train_data = dataset.train\n",
        "        self.test_data = dataset.test\n",
        "        # self.test_for_eval = dataset.test_for_eval\n",
        "        self.num_user = dataset.num_user\n",
        "        self.num_item = dataset.num_item\n",
        "        self.neg = dataset.neg\n",
        "        self.factors = factors\n",
        "        self.factors_Text = text_factors\n",
        "        self.learning_rate = learning_rate\n",
        "        self.reg = reg\n",
        "        self.init_mean = init_mean\n",
        "        self.init_stdev = init_stdev\n",
        "        # self.alpha = alpha\n",
        "        self.alpha = nn.Parameter(torch.tensor(alpha)).to(DEVICE).requires_grad_()\n",
        "\n",
        "        self.user_review_embeds = dataset.user_review_embeds.to(DEVICE)\n",
        "        self.poi_review_embeds = dataset.poi_review_embeds.to(DEVICE)\n",
        "\n",
        "        # 사용자와 아이템의 잠재 요인을 초기화합니다.\n",
        "        self.embed_user = torch.normal(mean=self.init_mean * torch.ones(self.num_user, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "        self.embed_item = torch.normal(mean=self.init_mean * torch.ones(self.num_item, self.factors), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "\n",
        "        self.beta_items = torch.normal(mean=self.init_mean * torch.ones(self.num_item, 1), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "        self.text_bias = torch.normal(mean=self.init_mean * torch.ones(768, 1), std=self.init_stdev).to(DEVICE).requires_grad_()\n",
        "\n",
        "        # Adam optimizer를 초기화합니다.\n",
        "        self.mf_optim = optim.Adam([self.embed_user, self.embed_item, self.beta_items, self.text_bias], lr=self.learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        '''\n",
        "        MF-BPR 모델의 forward pass입니다.\n",
        "        Args:\n",
        "            u: 사용자 ID.\n",
        "            i: 긍정적인 아이템 ID.\n",
        "            j: 부정적인 아이템 ID.\n",
        "        Returns:\n",
        "            y_ui: 사용자와 긍정적인 아이템 간의 예측 점수.\n",
        "            y_uj: 사용자와 부정적인 아이템 간의 예측 점수.\n",
        "            loss: BPR 손실.\n",
        "        '''\n",
        "        # 사용자와 긍정적인 아이템 간의 예측 점수 계산\n",
        "\n",
        "        user_latent_factor = self.embed_user[u]\n",
        "        user_text_factors = self.user_review_embeds[u] / math.sqrt(786)\n",
        "        alpha = self.alpha\n",
        "\n",
        "\n",
        "        i_bias = self.beta_items[i] # batch * 1\n",
        "        j_bias = self.beta_items[j] # batch * 1\n",
        "\n",
        "        i_text_factors = self.poi_review_embeds[i] # batch * 768\n",
        "        j_text_factors = self.poi_review_embeds[j] # batch * 768\n",
        "\n",
        "        i_latent_factors = self.embed_item[i]\n",
        "        j_latent_factors = self.embed_item[j]\n",
        "\n",
        "        diff_latent_factors = i_latent_factors - j_latent_factors # batch * latent\n",
        "        diff_text_factors = (i_text_factors - j_text_factors) / math.sqrt(768) # batch * 768\n",
        "\n",
        "        if diff_text_factors.shape[0] == 768: # [768], eval set이라면\n",
        "            user_latent_factor = user_latent_factor.unsqueeze(0)\n",
        "            user_text_factors = user_text_factors.unsqueeze(0)\n",
        "            diff_text_factors = diff_text_factors.unsqueeze(0) # [1, text_emb]\n",
        "            diff_latent_factors = diff_latent_factors.unsqueeze(0) # [ 1, latent_emb]\n",
        "\n",
        "        latent_factor = (user_latent_factor * diff_latent_factors).sum(dim=-1).unsqueeze(-1)\n",
        "        text_factor = (user_text_factors * diff_text_factors).sum(dim=-1).unsqueeze(-1)\n",
        "\n",
        "        u_i_score = alpha * latent_factor + (1 - alpha) * text_factor\n",
        "        text_bias = diff_text_factors.mm(self.text_bias)\n",
        "\n",
        "        x_uij = i_bias - j_bias + u_i_score + text_bias\n",
        "\n",
        "        # 정규화 항 계산\n",
        "        # BPR 손실 계산\n",
        "        loss = -torch.sum(torch.log(torch.sigmoid(x_uij.unsqueeze(0))))\n",
        "        return loss\n",
        "\n",
        "    def build_model(self, epoch=30, batch_size=32, topK = 10):\n",
        "        '''\n",
        "        MF-BPR 모델을 구축하고 학습합니다.\n",
        "        Args:\n",
        "            epoch (int): 학습의 최대 반복 횟수.\n",
        "            num_thread (int): 병렬 실행을 위한 스레드 수.\n",
        "            batch_size (int): 학습용 배치 크기.\n",
        "        '''\n",
        "        data_loader = DataLoader(self.dataset, batch_size=batch_size)\n",
        "\n",
        "        print(\"Training MF-BPR with: learning_rate=%.4f, regularization=%.7f, factors=%d, #epoch=%d, batch_size=%d.\"\n",
        "               % (self.learning_rate, self.reg, self.factors, epoch, batch_size))\n",
        "        t1 = time.time()\n",
        "\n",
        "        max_hit, max_precision, max_recall, max_recall_epoch, max_precision_epoch, max_hit_epoch = 0,0,0,0,0,0\n",
        "        for epoc in range(epoch):\n",
        "            iter_loss = 0\n",
        "            count = 0\n",
        "            for s, (users, items_pos, items_neg) in enumerate(data_loader):\n",
        "                users = users.to(DEVICE)\n",
        "                items_pos = items_pos.to(DEVICE)\n",
        "                items_neg = items_neg.to(DEVICE)\n",
        "\n",
        "                count += 1\n",
        "                # 기울기 초기화\n",
        "                self.mf_optim.zero_grad()\n",
        "                # Forward pass를 통해 예측과 손실 계산\n",
        "                loss = self.forward(users, items_pos, items_neg)\n",
        "                iter_loss += loss\n",
        "                # Backward pass 및 파라미터 업데이트\n",
        "                loss.backward()\n",
        "                self.mf_optim.step()\n",
        "            t2 = time.time()\n",
        "\n",
        "            # 성능 측정 함수를 통해 HitRatio 및 NDCG를 계산\n",
        "            hits, recall, precision = self.evaluate_model(self.test_data, topK)\n",
        "            # eval_loss = 0\n",
        "            # for idx, (u, i, j) in enumerate(self.test_for_eval):\n",
        "            #     u, i, j = u.to(DEVICE), i.to(DEVICE), j.to(DEVICE)\n",
        "            #     loss = self.forward(u, i, j)\n",
        "            #     eval_loss += loss\n",
        "            # total_samples = len(self.test_for_eval)\n",
        "            # eval_loss = eval_loss / total_samples if total_samples > 0 else 0\n",
        "            # iter_loss = iter_loss / count / batch_size\n",
        "            print(f\"epoch={epoc}, train_loss = {iter_loss:.6} [{int(t2-t1)}s] HitRatio@{topK} = {hits:.6}, RECAll@{topK} = {recall:.6}, PRECISION@{topK} = {precision:.6} [{int(time.time()-t2)}s], alpha: {alpha}\")\n",
        "            t1 = time.time()\n",
        "            if precision > max_precision:\n",
        "                max_precision = precision\n",
        "                max_precision_epoch = epoc\n",
        "            if recall > max_recall:\n",
        "                max_recall = recall\n",
        "                max_recall_epoch = epoc\n",
        "            if hits > max_hit:\n",
        "                max_hit = hits\n",
        "                max_hit_epoch = epoc\n",
        "            t1 = time.time()\n",
        "\n",
        "        #save_perform(reg, batch_size, latent_factors, text_factors, epoc, learning_rate, max_hit, max_hit_epoch, max_recall, max_recall_epoch, max_precision, max_precision_epoch, alpha)\n",
        "\n",
        "\n",
        "    def evaluate_model(self, test, K):\n",
        "        \"\"\"\n",
        "        Top-K 추천의 성능(Hit_Ratio, NDCG)을 평가합니다.\n",
        "        반환값: 각 테스트 상호작용의 점수.\n",
        "        \"\"\"\n",
        "        user_latent_factor = self.embed_user # batch * latent\n",
        "        item_latent_factors = self.embed_item # batch * latent\n",
        "\n",
        "        user_text_factors = self.user_review_embeds / math.sqrt(768) # batch * latent\n",
        "        item_text_factors = self.poi_review_embeds / math.sqrt(768)# batch * 768\n",
        "\n",
        "\n",
        "        latent_score_matrix = torch.mm(user_latent_factor, item_latent_factors.t())\n",
        "        text_score_matrix = torch.mm(user_text_factors, item_text_factors.t())\n",
        "\n",
        "        score_matrix = self.alpha * latent_score_matrix + (1-self.alpha) * text_score_matrix\n",
        "\n",
        "        item_bias = self.beta_items.squeeze()\n",
        "        item_bias = item_bias.view(1, -1)\n",
        "        score_matrix = score_matrix + item_bias\n",
        "\n",
        "\n",
        "        top_scores, top_indicies = torch.topk(score_matrix, K, dim=1)\n",
        "\n",
        "        hits = 0\n",
        "        sum_recall = 0\n",
        "        sum_precision = 0\n",
        "        for u,hist in enumerate(test):\n",
        "            set_topk = set(i.item() for i in (top_indicies[u]))\n",
        "            set_hist = set(hist)\n",
        "\n",
        "            if set_hist & set_topk:\n",
        "                hits += 1\n",
        "            sum_precision += len(set_hist & set_topk) / len(set_topk)\n",
        "            sum_recall += len(set_hist & set_topk) / len(set_hist)\n",
        "\n",
        "        return hits / len(test), sum_recall / len(test), sum_precision / len(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qBEb-Lu8jwKe"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xF-neqBr-J_k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15923 15923 15923 15923\n",
            "14585 14585 14585 14585\n",
            "calculate_distance time : 588\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(30)\n",
        "yelp = Yelp()\n",
        "# with open('/content/drive/MyDrive/학교/졸업작품/yelp.pkl', 'wb') as f:\n",
        "#     pickle.dump(yelp, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "cij8g_bdd81d"
      },
      "outputs": [],
      "source": [
        "# 기존 한 유저당 1개의 긍정, 부정 데이터셋\n",
        "# len(yelp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hX63rKoAy65K"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/학교/졸업작품/yelp.pkl', 'rb') as f:\n",
        "#     yelp = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voOnkC-JdAKe",
        "outputId": "a19eabdc-2424-495e-f14a-c6aa04f9ba98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "319337"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 개선 한 유저당 n개의 긍정, 부정 데이터셋(n = 방문한 모든 POI)\n",
        "len(yelp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ip0jc-bvwS7q",
        "outputId": "17a1b63a-ec5c-4077-c0ce-3ffabf949ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thread num:  20\n",
            "#factors: 768, lr: 0.000500, reg: 0.000100, batch_size: 1024\n",
            "Training MF-BPR with: learning_rate=0.0005, regularization=0.0001000, factors=768, #epoch=100, batch_size=1024.\n",
            "epoch=0, loss = 221349.03725646774[4s] HitRatio@10 = 0.19054198329460528, RECAll@10 = 0.03196700355648323, PRECISION@10 = 0.022746969792125003 [1s]\n",
            "epoch=1, loss = 216578.94215016838[3s] HitRatio@10 = 0.21999623186585443, RECAll@10 = 0.03530030577954103, PRECISION@10 = 0.02784023111222869 [1s]\n",
            "epoch=2, loss = 175591.39470252808[3s] HitRatio@10 = 0.21597688877724047, RECAll@10 = 0.034486460430209594, PRECISION@10 = 0.027432016579791288 [1s]\n",
            "epoch=3, loss = 120578.40993396065[4s] HitRatio@10 = 0.2248948062551027, RECAll@10 = 0.03622490561457287, PRECISION@10 = 0.02860013816491981 [1s]\n",
            "epoch=4, loss = 99169.52326332314[3s] HitRatio@10 = 0.2276581046285248, RECAll@10 = 0.03677317707539808, PRECISION@10 = 0.02909627582742066 [1s]\n",
            "epoch=5, loss = 90367.37333961303[3s] HitRatio@10 = 0.23701563775670415, RECAll@10 = 0.038306289940867336, PRECISION@10 = 0.030421403001948163 [1s]\n",
            "epoch=6, loss = 85176.20847856485[3s] HitRatio@10 = 0.2437982792187402, RECAll@10 = 0.039906013376661095, PRECISION@10 = 0.03154556302204502 [1s]\n",
            "epoch=7, loss = 80189.96651520632[4s] HitRatio@10 = 0.2510205363310934, RECAll@10 = 0.04108990773784892, PRECISION@10 = 0.0324624756641352 [1s]\n",
            "epoch=8, loss = 75264.31769040666[3s] HitRatio@10 = 0.2569867487282547, RECAll@10 = 0.04237263545388378, PRECISION@10 = 0.03327262450543414 [1s]\n",
            "epoch=9, loss = 70457.63644944679[3s] HitRatio@10 = 0.26408340136908875, RECAll@10 = 0.0438501837194209, PRECISION@10 = 0.034553790114930066 [1s]\n",
            "epoch=10, loss = 66025.44994085506[4s] HitRatio@10 = 0.2701124160020097, RECAll@10 = 0.04508658214745807, PRECISION@10 = 0.035100169566038605 [1s]\n",
            "epoch=11, loss = 61341.69102456695[3s] HitRatio@10 = 0.27545060604157506, RECAll@10 = 0.046270651806410774, PRECISION@10 = 0.03599824153740088 [1s]\n",
            "epoch=12, loss = 57251.79033566883[3s] HitRatio@10 = 0.28832506437229166, RECAll@10 = 0.048864267252774034, PRECISION@10 = 0.03809583621177168 [1s]\n",
            "epoch=13, loss = 53229.95512732688[3s] HitRatio@10 = 0.2907743515669158, RECAll@10 = 0.04969883852269498, PRECISION@10 = 0.03851033096778501 [1s]\n",
            "epoch=14, loss = 49911.56276793186[4s] HitRatio@10 = 0.2984362243295861, RECAll@10 = 0.05152984633900592, PRECISION@10 = 0.03999246373171155 [1s]\n",
            "epoch=15, loss = 46378.779245363454[3s] HitRatio@10 = 0.30390001884067075, RECAll@10 = 0.05243549966299938, PRECISION@10 = 0.04072096966652307 [1s]\n",
            "epoch=16, loss = 43745.06940516323[3s] HitRatio@10 = 0.3096150222947937, RECAll@10 = 0.05395473671452841, PRECISION@10 = 0.04171952521510073 [1s]\n",
            "epoch=17, loss = 40519.943019983555[3s] HitRatio@10 = 0.3130691452615713, RECAll@10 = 0.05463548265163436, PRECISION@10 = 0.04207749795893046 [1s]\n",
            "epoch=18, loss = 37807.92901289953[4s] HitRatio@10 = 0.31840733530113674, RECAll@10 = 0.05602286685285331, PRECISION@10 = 0.04311373484896389 [1s]\n",
            "epoch=19, loss = 35429.35936360813[3s] HitRatio@10 = 0.3222382716824719, RECAll@10 = 0.057079316076352124, PRECISION@10 = 0.043459147145641676 [1s]\n",
            "epoch=20, loss = 33319.059591111305[3s] HitRatio@10 = 0.3278276706650757, RECAll@10 = 0.05858880807900504, PRECISION@10 = 0.04433209822270018 [1s]\n",
            "epoch=21, loss = 31039.700758117873[3s] HitRatio@10 = 0.3296489355021039, RECAll@10 = 0.058767321607615684, PRECISION@10 = 0.04469635119010582 [1s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m bpr \u001b[38;5;241m=\u001b[39m DistBPR(yelp, latent_factors, learning_rate, reg, init_mean, init_stdev)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#bpr = TextBPR(yelp, latent_factors, text_factors, learning_rate, reg, init_mean, init_stdev, alpha)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#bpr = FreqBPR(yelp, latent_factors, learning_rate, reg, init_mean, init_stdev)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mbpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 학습된 가중치 저장\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# np.save(\"out/u\"+str(learning_rate)+\".npy\", bpr.U.detach().numpy())\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# np.save(\"out/v\"+str(learning_rate)+\".npy\", bpr.V.detach().numpy())\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[47], line 86\u001b[0m, in \u001b[0;36mDistBPR.build_model\u001b[1;34m(self, epoch, batch_size, topK)\u001b[0m\n\u001b[0;32m     84\u001b[0m     iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Backward pass 및 파라미터 업데이트\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmf_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    np.random.seed(30)  # 시드 설정을 통해 일관된 결과를 얻기 위해\n",
        "\n",
        "    #yelp = Yelp()\n",
        "\n",
        "    latent_factors = 768  # 잠재요인 수\n",
        "    text_factors = 768  # 텍스트 잠재요인 수\n",
        "    learning_rate = 0.0005  # 학습률\n",
        "    reg = 1e-4  # 정규화 계수\n",
        "    init_mean = 0  # 초기 가중치 평균\n",
        "    init_stdev = 0.001  # 초기 가중치 표준편차\n",
        "    epoch = 100  # 최대 반복 횟수\n",
        "    batch_size = 1024  # 미니배치 크기\n",
        "    alpha = 0.75\n",
        "    num_thread = mp.cpu_count() # 사용할 스레드 수\n",
        "    print(\"thread num: \", num_thread)\n",
        "    K = 10\n",
        "    print(\"#factors: %d, lr: %f, reg: %f, batch_size: %d\" % (latent_factors, learning_rate, reg, batch_size))\n",
        "\n",
        "    # MF-BPR 모델 생성 및 학습\n",
        "    bpr = MFbpr(yelp, latent_factors, learning_rate, reg, init_mean, init_stdev)\n",
        "    #bpr = DistBPR(yelp, latent_factors, learning_rate, reg, init_mean, init_stdev)\n",
        "    #bpr = TextBPR(yelp, latent_factors, text_factors, learning_rate, reg, init_mean, init_stdev, alpha)\n",
        "    #bpr = FreqBPR(yelp, latent_factors, learning_rate, reg, init_mean, init_stdev)\n",
        "\n",
        "    bpr.build_model(epoch, batch_size=batch_size, topK = K)\n",
        "\n",
        "    # 학습된 가중치 저장\n",
        "    # np.save(\"out/u\"+str(learning_rate)+\".npy\", bpr.U.detach().numpy())\n",
        "    # np.save(\"out/v\"+str(learning_rate)+\".npy\", bpr.V.detach().numpy())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
